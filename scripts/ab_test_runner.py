#!/usr/bin/env python3
"""
Teste A/B: AnÃ¡lise do impacto do ISR na detecÃ§Ã£o de alucinaÃ§Ãµes
- Teste A: SEM validaÃ§Ã£o (baseline)
- Teste B: COM validaÃ§Ã£o + ISR
"""

import json
import os
import sys
from pathlib import Path
from datetime import datetime
import shutil
from typing import Dict, List, Tuple

# Adicionar src ao path
sys.path.insert(0, str(Path(__file__).parent))

from src.utils.config import settings
from src.utils.logger import setup_logger
from src.loaders.artifacts import ArtifactLoader
from src.services.model_executor import ModelExecutor
from src.models.responses import RespostaCaso

logger = setup_logger(__name__)


class ABTestRunner:
    """Runner para testes A/B com e sem validaÃ§Ã£o"""
    
    def __init__(self):
        self.results_a = {"test": "A (SEM validaÃ§Ã£o)", "casos": []}
        self.results_b = {"test": "B (COM validaÃ§Ã£o)", "casos": []}
        self.artifact_loader = ArtifactLoader()
        
    def prepare_test_a(self):
        """Prepara Teste A: SEM dados de validaÃ§Ã£o"""
        logger.info("=== PREPARANDO TESTE A (SEM VALIDAÃ‡ÃƒO) ===")
        
        # Carrega artefatos normais
        politicas = self.artifact_loader.carregar_politicas()
        clientes = self.artifact_loader.carregar_clientes()
        casos = self.artifact_loader.carregar_casos_teste()
        prompt = self.artifact_loader.carregar_prompt_template()
        
        # Aqui vocÃª pode remover dados de validaÃ§Ã£o do prompt se necessÃ¡rio
        # Por agora, usamos o prompt original
        
        return politicas, clientes, casos, prompt
    
    def prepare_test_b(self):
        """Prepara Teste B: COM dados de validaÃ§Ã£o + matriz"""
        logger.info("=== PREPARANDO TESTE B (COM VALIDAÃ‡ÃƒO + ISR) ===")
        
        # Carrega artefatos com validaÃ§Ã£o
        politicas = self.artifact_loader.carregar_politicas()
        clientes = self.artifact_loader.carregar_clientes()
        casos = self.artifact_loader.carregar_casos_teste()
        prompt = self.artifact_loader.carregar_prompt_template()
        matriz = self.artifact_loader.carregar_matriz_validacao()
        
        return politicas, clientes, casos, prompt, matriz
    
    async def run_test_a(self):
        """Executa Teste A"""
        logger.info("\n" + "="*80)
        logger.info("INICIANDO TESTE A: SEM VALIDAÃ‡ÃƒO")
        logger.info("="*80)
        
        politicas, clientes, casos, prompt = self.prepare_test_a()
        executor = ModelExecutor(settings)
        
        # Contexto sem validaÃ§Ã£o
        context_a = {
            "politicas": politicas,
            "clientes": clientes,
            "prompt": prompt,
            "modo": "baseline",
            "include_validation": False
        }
        
        total_casos = min(len(casos), 5)  # Primeiros 5 para teste rÃ¡pido
        for i, caso in enumerate(casos[:total_casos]):
            logger.info(f"Executando caso {i+1}/{total_casos}: {caso.caso_id}")
            try:
                resposta = await executor.executar_caso(caso, context_a)
                self.results_a["casos"].append({
                    "caso_id": caso.caso_id,
                    "tipo": caso.tipo_cenario,
                    "decisao_esperada": caso.output_esperado.get("decisao"),
                    "decisao_ia": resposta.get("decisao", "N/A"),
                    "acertou": resposta.get("decisao") == caso.output_esperado.get("decisao"),
                    "confianca": resposta.get("confianca", 0),
                    "isr": None  # Teste A nÃ£o tem ISR
                })
            except Exception as e:
                logger.error(f"Erro no caso {caso.caso_id}: {e}")
                self.results_a["casos"].append({
                    "caso_id": caso.caso_id,
                    "tipo": caso.tipo_cenario,
                    "erro": str(e)
                })
    
    async def run_test_b(self):
        """Executa Teste B"""
        logger.info("\n" + "="*80)
        logger.info("INICIANDO TESTE B: COM VALIDAÃ‡ÃƒO + ISR")
        logger.info("="*80)
        
        politicas, clientes, casos, prompt, matriz = self.prepare_test_b()
        executor = ModelExecutor(settings)
        
        # Contexto com validaÃ§Ã£o
        context_b = {
            "politicas": politicas,
            "clientes": clientes,
            "prompt": prompt,
            "matriz_validacao": matriz,
            "modo": "com_validacao",
            "include_validation": True
        }
        
        total_casos = min(len(casos), 5)  # Primeiros 5 para teste rÃ¡pido
        for i, caso in enumerate(casos[:total_casos]):
            logger.info(f"Executando caso {i+1}/{total_casos}: {caso.caso_id}")
            try:
                resposta = await executor.executar_caso(caso, context_b)
                self.results_b["casos"].append({
                    "caso_id": caso.caso_id,
                    "tipo": caso.tipo_cenario,
                    "decisao_esperada": caso.output_esperado.get("decisao"),
                    "decisao_ia": resposta.get("decisao", "N/A"),
                    "acertou": resposta.get("decisao") == caso.output_esperado.get("decisao"),
                    "confianca": resposta.get("confianca", 0),
                    "isr": resposta.get("isr", None)
                })
            except Exception as e:
                logger.error(f"Erro no caso {caso.caso_id}: {e}")
                self.results_b["casos"].append({
                    "caso_id": caso.caso_id,
                    "tipo": caso.tipo_cenario,
                    "erro": str(e)
                })
    
    def calculate_metrics(self, results: Dict) -> Dict:
        """Calcula mÃ©tricas: Recall, AcurÃ¡cia, PrecisÃ£o, ISR mÃ©dio"""
        casos = results["casos"]
        
        # Filtra casos sem erro
        valid_casos = [c for c in casos if "erro" not in c]
        
        if not valid_casos:
            return {
                "total": len(casos),
                "vÃ¡lidos": 0,
                "acurÃ¡cia": 0,
                "recall": 0,
                "precisÃ£o": 0,
                "isr_mÃ©dio": 0
            }
        
        # TP: acertou, TN: errou (para alucinaÃ§Ã£o Ã© TP se detectou)
        tp = sum(1 for c in valid_casos if c.get("acertou", False))
        total = len(valid_casos)
        acurÃ¡cia = tp / total if total > 0 else 0
        
        # Recall: dos casos que deviam ser detectados, quantos foram?
        # Neste contexto: quantos erros/alucinaÃ§Ãµes foram detectados?
        recall = tp / total if total > 0 else 0
        
        # PrecisÃ£o: dos que foram detectados como erro, quantos eram reais?
        precisÃ£o = tp / total if total > 0 else 0
        
        # ISR mÃ©dio (Teste B)
        isr_values = [c.get("isr") for c in valid_casos if c.get("isr") is not None]
        isr_mÃ©dio = sum(isr_values) / len(isr_values) if isr_values else 0
        
        return {
            "total": len(casos),
            "vÃ¡lidos": len(valid_casos),
            "tp": tp,
            "acurÃ¡cia": round(acurÃ¡cia * 100, 2),
            "recall": round(recall * 100, 2),
            "precisÃ£o": round(precisÃ£o * 100, 2),
            "isr_mÃ©dio": round(isr_mÃ©dio, 4),
            "confianca_mÃ©dia": round(sum(c.get("confianca", 0) for c in valid_casos) / len(valid_casos), 2) if valid_casos else 0
        }
    
    def generate_report(self):
        """Gera relatÃ³rio comparativo A/B"""
        metrics_a = self.calculate_metrics(self.results_a)
        metrics_b = self.calculate_metrics(self.results_b)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_path = Path("outputs/ab_test") / f"ab_test_report_{timestamp}.md"
        report_path.parent.mkdir(parents=True, exist_ok=True)
        
        report = f"""# RelatÃ³rio Teste A/B: Impacto da ValidaÃ§Ã£o e ISR

**Data**: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}

## Resumo Executivo

- **Teste A (Baseline SEM validaÃ§Ã£o)**: {metrics_a['vÃ¡lidos']} casos vÃ¡lidos
- **Teste B (COM validaÃ§Ã£o + ISR)**: {metrics_b['vÃ¡lidos']} casos vÃ¡lidos

## ComparaÃ§Ã£o de MÃ©tricas

| MÃ©trica | Teste A (SEM validaÃ§Ã£o) | Teste B (COM validaÃ§Ã£o) | DiferenÃ§a |
|---------|------------------------|----------------------|-----------|
| **AcurÃ¡cia** | {metrics_a['acurÃ¡cia']}% | {metrics_b['acurÃ¡cia']}% | {metrics_b['acurÃ¡cia'] - metrics_a['acurÃ¡cia']:+.2f}% |
| **Recall** | {metrics_a['recall']}% | {metrics_b['recall']}% | {metrics_b['recall'] - metrics_a['recall']:+.2f}% |
| **PrecisÃ£o** | {metrics_a['precisÃ£o']}% | {metrics_b['precisÃ£o']}% | {metrics_b['precisÃ£o'] - metrics_a['precisÃ£o']:+.2f}% |
| **ConfianÃ§a MÃ©dia** | {metrics_a['confianca_mÃ©dia']} | {metrics_b['confianca_mÃ©dia']} | {metrics_b['confianca_mÃ©dia'] - metrics_a['confianca_mÃ©dia']:+.2f} |
| **ISR MÃ©dio** | N/A | {metrics_b['isr_mÃ©dio']} | - |

## AnÃ¡lise de Resultados

### DetecÃ§Ã£o de AlucinaÃ§Ãµes

- **Teste A (Baseline)**: Recall = {metrics_a['recall']}% - Modelo detecta {metrics_a['recall']:.0f}% dos erros sem validaÃ§Ã£o
- **Teste B (Com ISR)**: Recall = {metrics_b['recall']}% - Modelo detecta {metrics_b['recall']:.0f}% dos erros com validaÃ§Ã£o

**Melhoria**: {metrics_b['recall'] - metrics_a['recall']:+.2f}% (ISR ajuda na detecÃ§Ã£o? {'SIM' if metrics_b['recall'] > metrics_a['recall'] else 'NÃƒO'})

### Impacto da ValidaÃ§Ã£o

- ISR MÃ©dio: {metrics_b['isr_mÃ©dio']} (esperado: > 0.85)
- A validaÃ§Ã£o {'MELHOROU' if metrics_b['acurÃ¡cia'] > metrics_a['acurÃ¡cia'] else 'PIOROU'} a acurÃ¡cia em {abs(metrics_b['acurÃ¡cia'] - metrics_a['acurÃ¡cia']):.2f}%

## ConclusÃµes

1. **ISR Ã© Ãºtil?** {('SIM - Melhoria de ' + str(metrics_b['recall'] - metrics_a['recall']) + '%') if metrics_b['recall'] > metrics_a['recall'] else 'NÃƒO - Sem melhoria significativa'}
2. **ValidaÃ§Ã£o reduz alucinaÃ§Ãµes?** {('SIM' if metrics_b['acurÃ¡cia'] > metrics_a['acurÃ¡cia'] else 'NÃƒO')}
3. **RecomendaÃ§Ã£o**: {'Incluir validaÃ§Ã£o nos prompts' if metrics_b['acurÃ¡cia'] > metrics_a['acurÃ¡cia'] else 'Revisar estratÃ©gia de validaÃ§Ã£o'}

## Detalhes por Caso (Teste B)

"""
        
        report += "\n| Caso | Tipo | Esperado | IA Decidiu | Acertou | ConfianÃ§a | ISR |\n"
        report += "|------|------|----------|-----------|---------|-----------|-----|\n"
        
        for caso in self.results_b["casos"]:
            if "erro" not in caso:
                report += f"| {caso['caso_id']} | {caso['tipo']} | {caso['decisao_esperada']} | {caso['decisao_ia']} | {'âœ“' if caso['acertou'] else 'âœ—'} | {caso['confianca']} | {caso.get('isr', 'N/A')} |\n"
        
        # Salvar relatÃ³rio
        with open(report_path, "w", encoding="utf-8") as f:
            f.write(report)
        
        logger.info(f"\nâœ… RelatÃ³rio salvo em: {report_path}")
        
        # TambÃ©m salva JSON com dados brutos
        json_path = report_path.parent / f"ab_test_results_{timestamp}.json"
        with open(json_path, "w", encoding="utf-8") as f:
            json.dump({
                "test_a": self.results_a,
                "test_b": self.results_b,
                "metrics_a": metrics_a,
                "metrics_b": metrics_b
            }, f, indent=2, ensure_ascii=False)
        
        logger.info(f"âœ… Dados brutos salvos em: {json_path}")
        
        # Print summary
        print("\n" + "="*80)
        print("RESULTADO FINAL DO TESTE A/B")
        print("="*80)
        print(f"\nðŸ“Š ACURÃCIA:")
        print(f"  Teste A (SEM validaÃ§Ã£o):  {metrics_a['acurÃ¡cia']}%")
        print(f"  Teste B (COM validaÃ§Ã£o):  {metrics_b['acurÃ¡cia']}%")
        print(f"  Melhoria: {metrics_b['acurÃ¡cia'] - metrics_a['acurÃ¡cia']:+.2f}%")
        
        print(f"\nðŸŽ¯ RECALL (DetecÃ§Ã£o de Erros):")
        print(f"  Teste A: {metrics_a['recall']}%")
        print(f"  Teste B: {metrics_b['recall']}%")
        print(f"  Melhoria: {metrics_b['recall'] - metrics_a['recall']:+.2f}%")
        
        print(f"\nðŸ“ˆ ISR MÃ‰DIO (Teste B):")
        print(f"  ISR: {metrics_b['isr_mÃ©dio']}")
        print(f"  Status: {'âœ“ ADEQUADO (>0.85)' if metrics_b['isr_mÃ©dio'] > 0.85 else 'âœ— INADEQUADO (<0.85)'}")
        
        return metrics_a, metrics_b


async def main():
    """Main entry point"""
    runner = ABTestRunner()
    
    print("\n" + "="*80)
    print("TESTE A/B: IMPACTO DA VALIDAÃ‡ÃƒO E ISR NA DETECÃ‡ÃƒO DE ALUCINAÃ‡Ã•ES")
    print("="*80)
    
    # Executar testes
    await runner.run_test_a()
    await runner.run_test_b()
    
    # Gerar relatÃ³rio
    metrics_a, metrics_b = runner.generate_report()


if __name__ == "__main__":
    import asyncio
    asyncio.run(main())
