{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": "# Avaliação DESAFIADORA: Modelo Padrão vs ISR REAL\n\n## Sextant Banking Edition - Teste de Robustez com Métricas Completas\n\n**Autor:** SK-Crossroads  \n**Data:** Janeiro 2026  \n**Versão:** 5.2 (Casos Desafiadores + ISR REAL)\n\n---\n\n### O que esta versão testa\n\n| Tipo de Armadilha | Exemplo | Decisão Esperada |\n|-------------------|---------|------------------|\n| Score alto + default recente | Score 820, default há 1 mês | NEGADA |\n| Score bom + endividamento oculto | Score 750, endiv. 92% | NEGADA |\n| Score baixo + cliente excelente | Score 580, 25 anos sem problemas | ANALISE_GERENCIAL |\n| PEP não declarado | Secretária de Estado, pep=false | ANALISE_GERENCIAL |\n| Renda incompatível | Estagiário com 45k/mês | ANALISE_GERENCIAL |\n| Cliente sancionado | Score perfeito mas sancionado=true | NEGADA |\n| Menor de idade | Nascido em 2010 | NEGADA |\n| Saldo negativo | Score bom, saldo=-5000 | NEGADA |\n\n### Comparação\n\n- **Modelo Padrão**: Apenas LLM com políticas do banco\n- **Modelo + ISR REAL**: LLM + `src/tools/isr_auditor.py` (Information Sufficiency Ratio)\n\n### ISR Real - 4 Patches Implementados\n\n1. **Laplace Smoothing** (PROB_FLOOR): Evita divisão por zero\n2. **One-Sided Clipping**: Foco na perda de informação\n3. **Hard Veto**: Detecta instabilidade severa\n4. **Success Shortcut**: Bypass para alta confiança"
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": "import os\nimport sys\nimport json\nimport math\nimport numpy as np\nfrom pathlib import Path\nfrom collections import Counter\nfrom typing import List, Dict, Any\nfrom datetime import datetime, date\nfrom copy import deepcopy\nfrom dotenv import load_dotenv\n\ndef find_project_root():\n    current = Path.cwd()\n    if current.name == \"notebooks\":\n        return current.parent\n    for parent in [current] + list(current.parents):\n        if (parent / \"data\").exists():\n            return parent\n    return Path(\"/home/dumoura/Kunumi/Hallucinations_ISR_V4\")\n\nPROJECT_ROOT = find_project_root()\nsys.path.insert(0, str(PROJECT_ROOT))\nload_dotenv(PROJECT_ROOT / \".env\")\n\nprint(f\"[OK] Projeto: {PROJECT_ROOT}\")\nprint(f\"[INFO] Versão 5.2: CASOS DESAFIADORES + ISR REAL\")\nprint(f\"[INFO] ISR usando implementação de src/tools/isr_auditor.py\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not OPENAI_API_KEY:\n",
    "    raise ValueError(\"OPENAI_API_KEY não encontrada!\")\n",
    "\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "MODEL_NAME = \"gpt-4o-mini\"\n",
    "\n",
    "print(f\"[OK] Modelo: {MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Carregando Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": "# Carregar políticas e casos desafiadores\nwith open(PROJECT_ROOT / \"data\" / \"raw\" / \"banco_politicas_diretrizes.md\", \"r\", encoding=\"utf-8\") as f:\n    POLITICAS_BANCO = f.read()\n\nwith open(PROJECT_ROOT / \"data\" / \"test\" / \"clientes_teste_desafiadores.json\", \"r\", encoding=\"utf-8\") as f:\n    dados_desafiadores = json.load(f)\n\nclientes_desafiadores = dados_desafiadores[\"clientes\"]\nprint(f\"[OK] {len(clientes_desafiadores)} casos desafiadores carregados\")\n\n# Distribuição\ndecisoes = Counter(c.get(\"_ground_truth\", {}).get(\"decisao_esperada\", \"N/A\") for c in clientes_desafiadores)\nprint(f\"\\nDistribuição de decisões esperadas:\")\nfor dec, count in sorted(decisoes.items()):\n    print(f\"  - {dec}: {count}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparar dataset\n",
    "def preparar_cliente_para_modelo(cliente: Dict) -> Dict:\n",
    "    return {k: v for k, v in cliente.items() if not k.startswith(\"_\")}\n",
    "\n",
    "def extrair_ground_truth(cliente: Dict) -> Dict:\n",
    "    gt = cliente.get(\"_ground_truth\", {})\n",
    "    return {\n",
    "        \"decisao_esperada\": gt.get(\"decisao_esperada\", \"NEGADA\"),\n",
    "        \"motivo\": gt.get(\"motivo\", \"Não especificado\"),\n",
    "        \"tipo_teste\": gt.get(\"tipo_teste\", \"unknown\")\n",
    "    }\n",
    "\n",
    "dataset = []\n",
    "for cliente in clientes_desafiadores:\n",
    "    dataset.append({\n",
    "        \"cliente_para_modelo\": preparar_cliente_para_modelo(cliente),\n",
    "        \"cliente_original\": cliente,\n",
    "        \"ground_truth\": extrair_ground_truth(cliente),\n",
    "        \"cliente_id\": cliente[\"cliente_id\"]\n",
    "    })\n",
    "\n",
    "print(f\"[OK] Dataset preparado: {len(dataset)} casos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Funções do Modelo Padrão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def criar_system_prompt(politicas: str) -> str:\n",
    "    return f\"\"\"Você é um analista de crédito do Banco S.A.\n",
    "\n",
    "Analise solicitações de crédito seguindo as políticas do banco.\n",
    "\n",
    "ATENÇÃO ESPECIAL:\n",
    "- Verifique TODOS os campos, não apenas o score\n",
    "\n",
    "---\n",
    "\n",
    "# POLÍTICAS DO BANCO\n",
    "\n",
    "{politicas[:18000]}\n",
    "\"\"\"\n",
    "\n",
    "def criar_prompt_analise(cliente: Dict) -> str:\n",
    "    cliente_json = json.dumps(cliente, indent=2, ensure_ascii=False, default=str)\n",
    "    return f\"\"\"# ANÁLISE DE CRÉDITO\n",
    "\n",
    "## Dados do Cliente\n",
    "```json\n",
    "{cliente_json}\n",
    "```\n",
    "\n",
    "## Instruções\n",
    "Analise TODOS os campos. Responda em JSON:\n",
    "{{\n",
    "  \"decisao\": \"APROVADA\" | \"NEGADA\" | \"ANALISE_GERENCIAL\",\n",
    "  \"justificativa\": \"<explicação>\",\n",
    "  \"alertas\": [\"<pontos de atenção>\"]\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "def extrair_json(texto: str) -> Dict:\n",
    "    if \"```json\" in texto:\n",
    "        inicio = texto.find(\"```json\") + 7\n",
    "        fim = texto.find(\"```\", inicio)\n",
    "        if fim > inicio:\n",
    "            try:\n",
    "                return json.loads(texto[inicio:fim].strip())\n",
    "            except:\n",
    "                pass\n",
    "    inicio = texto.find(\"{\")\n",
    "    fim = texto.rfind(\"}\")\n",
    "    if inicio >= 0 and fim > inicio:\n",
    "        try:\n",
    "            return json.loads(texto[inicio:fim+1])\n",
    "        except:\n",
    "            pass\n",
    "    return {\"erro\": \"JSON não encontrado\"}\n",
    "\n",
    "def chamar_modelo_padrao(cliente: Dict, system_prompt: str) -> Dict:\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=MODEL_NAME,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": criar_prompt_analise(cliente)}\n",
    "            ],\n",
    "            max_tokens=1024,\n",
    "            temperature=0.3\n",
    "        )\n",
    "        texto = response.choices[0].message.content\n",
    "        return {\"sucesso\": True, \"resposta_json\": extrair_json(texto)}\n",
    "    except Exception as e:\n",
    "        return {\"sucesso\": False, \"erro\": str(e)}\n",
    "\n",
    "print(\"[OK] Funções do modelo padrão definidas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": "---\n## 4. Modelo com ISR Real (Information Sufficiency Ratio)\n\nO ISR usa a implementação **real** de `src/tools/isr_auditor.py` com:\n\n1. **Laplace Smoothing** (PROB_FLOOR) - evita divisão por zero\n2. **One-Sided Clipping** - foco na perda de informação\n3. **Hard Veto** - detecta instabilidade severa\n4. **Success Shortcut** - bypass para alta confiança\n\nO processo:\n1. Modelo padrão gera uma decisão\n2. ISR verifica a consistência da decisão via permutações\n3. Se ISR detecta instabilidade → bloqueia a decisão"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": "# Importar a implementação REAL do ISR\nfrom src.tools.isr_auditor import SemanticISRAuditorTool\n\n# Criar instância do auditor ISR real\nisr_auditor = SemanticISRAuditorTool(\n    client=client,\n    model=MODEL_NAME,\n    target_confidence=0.95,\n    num_permutations=6,\n    clipping_b=12.0,\n    hard_veto_threshold=0.20\n)\n\nprint(f\"[OK] ISR Auditor Real carregado\")\nprint(f\"    - Modelo: {MODEL_NAME}\")\nprint(f\"    - Permutações: {isr_auditor.num_permutations}\")\nprint(f\"    - Target Confidence: {isr_auditor.target_confidence}\")\nprint(f\"    - Hard Veto Threshold: {isr_auditor.hard_veto_threshold}\")\n\n\ndef chamar_modelo_com_isr_real(cliente: Dict, system_prompt: str) -> Dict:\n    \"\"\"\n    Chama o modelo padrão e depois usa o ISR REAL para verificar a decisão.\n    \n    O ISR analisa a consistência da decisão via permutações de contexto.\n    Se detectar instabilidade severa, bloqueia a decisão.\n    \"\"\"\n    # Passo 1: Chamar modelo padrão\n    resp_modelo = chamar_modelo_padrao(cliente, system_prompt)\n    \n    if not resp_modelo[\"sucesso\"]:\n        return resp_modelo\n    \n    decisao_modelo = resp_modelo[\"resposta_json\"].get(\"decisao\", \"ERRO\")\n    justificativa_modelo = resp_modelo[\"resposta_json\"].get(\"justificativa\", \"\")\n    \n    # Passo 2: Construir contexto para o ISR\n    cliente_json = json.dumps(cliente, indent=2, ensure_ascii=False, default=str)\n    prompt_context = f\"\"\"POLÍTICAS DO BANCO:\n{system_prompt[:8000]}\n\nDADOS DO CLIENTE:\n{cliente_json}\n\nANÁLISE DO MODELO:\nDecisão: {decisao_modelo}\nJustificativa: {justificativa_modelo}\n\"\"\"\n    \n    # Passo 3: Usar ISR REAL para verificar a decisão\n    try:\n        isr_result_json = isr_auditor.audit(\n            prompt_context=prompt_context,\n            proposed_decision=decisao_modelo\n        )\n        isr_result = json.loads(isr_result_json)\n        \n        isr_decision = isr_result.get(\"decision\", \"ERRO\")\n        isr_metrics = isr_result.get(\"metrics\", {})\n        isr_reason = isr_result.get(\"reason\", \"\")\n        \n        # Passo 4: Aplicar decisão do ISR\n        if isr_decision == \"BLOQUEADO\":\n            # ISR detectou instabilidade - sobrescrever decisão\n            # Se o modelo aprovou mas ISR bloqueou → forçar análise gerencial\n            if decisao_modelo == \"APROVADA\":\n                decisao_final = \"ANALISE_GERENCIAL\"\n                justificativa_final = f\"ISR BLOQUEOU: {isr_reason}. Decisão original: {decisao_modelo}\"\n            else:\n                # Se já era negativa, manter\n                decisao_final = decisao_modelo\n                justificativa_final = f\"{justificativa_modelo} [ISR confirmou rejeição]\"\n            \n            return {\n                \"sucesso\": True,\n                \"resposta_json\": {\n                    \"decisao\": decisao_final,\n                    \"justificativa\": justificativa_final,\n                    \"alertas\": resp_modelo[\"resposta_json\"].get(\"alertas\", []) + [\"ISR_BLOQUEIO\"]\n                },\n                \"isr_aplicado\": True,\n                \"isr_acao\": \"BLOQUEIO\",\n                \"isr_metrics\": isr_metrics,\n                \"isr_reason\": isr_reason\n            }\n        else:\n            # ISR aprovou - manter decisão original\n            return {\n                \"sucesso\": True,\n                \"resposta_json\": resp_modelo[\"resposta_json\"],\n                \"isr_aplicado\": True,\n                \"isr_acao\": \"APROVADO\",\n                \"isr_metrics\": isr_metrics,\n                \"isr_reason\": isr_reason\n            }\n            \n    except Exception as e:\n        print(f\"    [ERRO ISR] {e}\")\n        # Em caso de erro, retornar resultado do modelo sem ISR\n        return {\n            \"sucesso\": True,\n            \"resposta_json\": resp_modelo[\"resposta_json\"],\n            \"isr_aplicado\": False,\n            \"isr_acao\": \"ERRO\",\n            \"isr_metrics\": {},\n            \"isr_reason\": str(e)\n        }\n\nprint(\"[OK] Função chamar_modelo_com_isr_real definida\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Executando Testes Comparativos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": "import time\n\nSYSTEM_PROMPT = criar_system_prompt(POLITICAS_BANCO)\n\nresultados_padrao = []\nresultados_isr = []\n\nprint(\"Executando testes comparativos com ISR REAL...\")\nprint(\"=\"*90)\nprint(\"NOTA: ISR Real usa permutações de contexto - mais lento mas mais preciso\")\nprint(\"=\"*90)\n\nfor i, item in enumerate(dataset):\n    cliente = item[\"cliente_para_modelo\"]\n    gt = item[\"ground_truth\"]\n    \n    print(f\"\\n[{i+1}/{len(dataset)}] {item['cliente_id']}\")\n    print(f\"    Esperado: {gt['decisao_esperada']} | Tipo: {gt['tipo_teste']}\")\n    \n    # Modelo Padrão\n    print(f\"    Padrão: \", end=\"\")\n    resp_padrao = chamar_modelo_padrao(cliente, SYSTEM_PROMPT)\n    if resp_padrao[\"sucesso\"]:\n        dec_padrao = resp_padrao[\"resposta_json\"].get(\"decisao\", \"ERRO\")\n        acertou_padrao = dec_padrao == gt[\"decisao_esperada\"]\n        print(f\"{dec_padrao} {'✓' if acertou_padrao else '✗'}\")\n    else:\n        dec_padrao = \"ERRO\"\n        acertou_padrao = False\n        print(f\"ERRO\")\n    \n    resultados_padrao.append({\n        \"cliente_id\": item[\"cliente_id\"],\n        \"tipo_teste\": gt[\"tipo_teste\"],\n        \"esperado\": gt[\"decisao_esperada\"],\n        \"obtido\": dec_padrao,\n        \"acertou\": acertou_padrao\n    })\n    \n    time.sleep(0.3)\n    \n    # Modelo com ISR REAL\n    print(f\"    ISR:    \", end=\"\")\n    resp_isr = chamar_modelo_com_isr_real(cliente, SYSTEM_PROMPT)\n    if resp_isr[\"sucesso\"]:\n        dec_isr = resp_isr[\"resposta_json\"].get(\"decisao\", \"ERRO\")\n        acertou_isr = dec_isr == gt[\"decisao_esperada\"]\n        isr_acao = resp_isr.get(\"isr_acao\", \"N/A\")\n        isr_metrics = resp_isr.get(\"isr_metrics\", {})\n        isr_value = isr_metrics.get(\"ISR\", \"N/A\")\n        print(f\"{dec_isr} {'✓' if acertou_isr else '✗'} (ISR: {isr_acao}, ISR={isr_value})\")\n    else:\n        dec_isr = \"ERRO\"\n        acertou_isr = False\n        isr_acao = \"ERRO\"\n        isr_metrics = {}\n        print(f\"ERRO\")\n    \n    resultados_isr.append({\n        \"cliente_id\": item[\"cliente_id\"],\n        \"tipo_teste\": gt[\"tipo_teste\"],\n        \"esperado\": gt[\"decisao_esperada\"],\n        \"obtido\": dec_isr,\n        \"acertou\": acertou_isr,\n        \"isr_acao\": isr_acao,\n        \"isr_metrics\": isr_metrics,\n        \"isr_reason\": resp_isr.get(\"isr_reason\", \"\")\n    })\n    \n    time.sleep(0.5)\n\nprint(\"\\n\" + \"=\"*90)\nprint(\"[OK] Testes concluídos!\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Calculando Métricas de ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcular_metricas_ml(resultados: List[Dict]) -> Dict:\n",
    "    \"\"\"\n",
    "    Calcula métricas de ML para classificação multiclasse.\n",
    "    \n",
    "    Para métricas binárias (precision, recall, F1):\n",
    "    - POSITIVO = APROVADA (conceder crédito)\n",
    "    - NEGATIVO = NEGADA ou ANALISE_GERENCIAL (não conceder automaticamente)\n",
    "    \"\"\"\n",
    "    valid = [r for r in resultados if r[\"obtido\"] != \"ERRO\"]\n",
    "    if not valid:\n",
    "        return {\"erro\": \"Sem resultados válidos\"}\n",
    "    \n",
    "    total = len(valid)\n",
    "    acertos = sum(1 for r in valid if r[\"acertou\"])\n",
    "    \n",
    "    # Accuracy geral\n",
    "    accuracy = acertos / total\n",
    "    \n",
    "    # Métricas binárias (APROVADA vs resto)\n",
    "    def to_binary(dec):\n",
    "        return 1 if dec == \"APROVADA\" else 0\n",
    "    \n",
    "    y_true = [to_binary(r[\"esperado\"]) for r in valid]\n",
    "    y_pred = [to_binary(r[\"obtido\"]) for r in valid]\n",
    "    \n",
    "    tp = sum(1 for t, p in zip(y_true, y_pred) if t == 1 and p == 1)\n",
    "    tn = sum(1 for t, p in zip(y_true, y_pred) if t == 0 and p == 0)\n",
    "    fp = sum(1 for t, p in zip(y_true, y_pred) if t == 0 and p == 1)  # Aprovou indevidamente\n",
    "    fn = sum(1 for t, p in zip(y_true, y_pred) if t == 1 and p == 0)  # Negou indevidamente\n",
    "    \n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    # Matriz de confusão multiclasse\n",
    "    classes = [\"APROVADA\", \"NEGADA\", \"ANALISE_GERENCIAL\"]\n",
    "    matriz = {}\n",
    "    for esp in classes:\n",
    "        matriz[esp] = {obt: 0 for obt in classes}\n",
    "    \n",
    "    for r in valid:\n",
    "        esp = r[\"esperado\"]\n",
    "        obt = r[\"obtido\"]\n",
    "        if esp in matriz and obt in matriz[esp]:\n",
    "            matriz[esp][obt] += 1\n",
    "    \n",
    "    return {\n",
    "        \"accuracy\": float(accuracy),\n",
    "        \"precision\": float(precision),\n",
    "        \"recall\": float(recall),\n",
    "        \"f1_score\": float(f1),\n",
    "        \"confusion_matrix_binary\": {\"TP\": tp, \"TN\": tn, \"FP\": fp, \"FN\": fn},\n",
    "        \"confusion_matrix_multi\": matriz,\n",
    "        \"total\": total,\n",
    "        \"acertos\": acertos,\n",
    "        \"false_positives\": fp  # Aprovações indevidas (risco!)\n",
    "    }\n",
    "\n",
    "metricas_padrao = calcular_metricas_ml(resultados_padrao)\n",
    "metricas_isr = calcular_metricas_ml(resultados_isr)\n",
    "\n",
    "print(\"[OK] Métricas calculadas!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Resultados: Modelo Padrão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"MODELO PADRÃO (LLM sem validação adicional)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "if \"erro\" not in metricas_padrao:\n",
    "    print(f\"\\nAcertos: {metricas_padrao['acertos']}/{metricas_padrao['total']}\")\n",
    "    \n",
    "    print(f\"\\nMétricas:\")\n",
    "    print(f\"  Accuracy:  {metricas_padrao['accuracy']:.1%}\")\n",
    "    print(f\"  Precision: {metricas_padrao['precision']:.1%}\")\n",
    "    print(f\"  Recall:    {metricas_padrao['recall']:.1%}\")\n",
    "    print(f\"  F1-Score:  {metricas_padrao['f1_score']:.1%}\")\n",
    "    \n",
    "    cm = metricas_padrao[\"confusion_matrix_binary\"]\n",
    "    print(f\"\\nMatriz de Confusão (Binária):\")\n",
    "    print(f\"  TP (Aprovou certo):    {cm['TP']}\")\n",
    "    print(f\"  TN (Negou certo):      {cm['TN']}\")\n",
    "    print(f\"  FP (RISCO - aprovação indevida): {cm['FP']}\")\n",
    "    print(f\"  FN (Negou indevidamente): {cm['FN']}\")\n",
    "else:\n",
    "    print(f\"ERRO: {metricas_padrao['erro']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": "---\n## 8. Resultados: Modelo com ISR REAL\n\n**ISR Real** (Information Sufficiency Ratio) da implementação `src/tools/isr_auditor.py`:\n- Verifica consistência via permutações de contexto\n- Calcula métricas: Delta, B2T, JS_Bound\n- Aplica Hard Veto para instabilidade severa"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": "print(\"=\"*70)\nprint(\"MODELO COM ISR REAL (implementação de src/tools/isr_auditor.py)\")\nprint(\"=\"*70)\n\nif \"erro\" not in metricas_isr:\n    print(f\"\\nAcertos: {metricas_isr['acertos']}/{metricas_isr['total']}\")\n    \n    print(f\"\\nMétricas de Classificação:\")\n    print(f\"  Accuracy:  {metricas_isr['accuracy']:.1%}\")\n    print(f\"  Precision: {metricas_isr['precision']:.1%}\")\n    print(f\"  Recall:    {metricas_isr['recall']:.1%}\")\n    print(f\"  F1-Score:  {metricas_isr['f1_score']:.1%}\")\n    \n    cm = metricas_isr[\"confusion_matrix_binary\"]\n    print(f\"\\nMatriz de Confusão (Binária):\")\n    print(f\"  TP (Aprovou certo):    {cm['TP']}\")\n    print(f\"  TN (Negou certo):      {cm['TN']}\")\n    print(f\"  FP (RISCO - aprovação indevida): {cm['FP']}\")\n    print(f\"  FN (Negou indevidamente): {cm['FN']}\")\n    \n    # Mostrar ações do ISR\n    acoes_isr = Counter(r.get(\"isr_acao\", \"N/A\") for r in resultados_isr)\n    print(f\"\\nAções do ISR Real:\")\n    for acao, count in sorted(acoes_isr.items()):\n        print(f\"  - {acao}: {count}\")\n    \n    # Mostrar estatísticas das métricas ISR\n    isr_values = [r.get(\"isr_metrics\", {}).get(\"ISR\", None) for r in resultados_isr]\n    isr_values = [v for v in isr_values if v is not None and v != 999.0]  # Filtrar valores válidos\n    \n    if isr_values:\n        print(f\"\\nEstatísticas do ISR (Information Sufficiency Ratio):\")\n        print(f\"  ISR Médio: {np.mean(isr_values):.4f}\")\n        print(f\"  ISR Mínimo: {np.min(isr_values):.4f}\")\n        print(f\"  ISR Máximo: {np.max(isr_values):.4f}\")\n        print(f\"  ISR >= 1.0 (aprovados): {sum(1 for v in isr_values if v >= 1.0)}\")\n        print(f\"  ISR < 1.0 (bloqueados): {sum(1 for v in isr_values if v < 1.0)}\")\nelse:\n    print(f\"ERRO: {metricas_isr['erro']}\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Comparação Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"COMPARAÇÃO: MODELO PADRÃO vs MODELO COM ISR\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if \"erro\" not in metricas_padrao and \"erro\" not in metricas_isr:\n",
    "    print(f\"\\n{'Métrica':<25} {'Modelo Padrão':>15} {'Modelo + ISR':>15} {'Diferença':>15}\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    # Accuracy\n",
    "    diff = metricas_isr['accuracy'] - metricas_padrao['accuracy']\n",
    "    print(f\"{'Accuracy':<25} {metricas_padrao['accuracy']:>14.1%} {metricas_isr['accuracy']:>14.1%} {diff:>+14.1%}\")\n",
    "    \n",
    "    # Precision\n",
    "    diff = metricas_isr['precision'] - metricas_padrao['precision']\n",
    "    print(f\"{'Precision':<25} {metricas_padrao['precision']:>14.1%} {metricas_isr['precision']:>14.1%} {diff:>+14.1%}\")\n",
    "    \n",
    "    # Recall\n",
    "    diff = metricas_isr['recall'] - metricas_padrao['recall']\n",
    "    print(f\"{'Recall':<25} {metricas_padrao['recall']:>14.1%} {metricas_isr['recall']:>14.1%} {diff:>+14.1%}\")\n",
    "    \n",
    "    # F1-Score\n",
    "    diff = metricas_isr['f1_score'] - metricas_padrao['f1_score']\n",
    "    print(f\"{'F1-Score':<25} {metricas_padrao['f1_score']:>14.1%} {metricas_isr['f1_score']:>14.1%} {diff:>+14.1%}\")\n",
    "    \n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    # False Positives (CRÍTICO)\n",
    "    diff = metricas_isr['false_positives'] - metricas_padrao['false_positives']\n",
    "    print(f\"{'False Positives (RISCO)':<25} {metricas_padrao['false_positives']:>15} {metricas_isr['false_positives']:>15} {diff:>+15}\")\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Análise\n",
    "    melhoria_accuracy = (metricas_isr['accuracy'] - metricas_padrao['accuracy']) / metricas_padrao['accuracy'] * 100 if metricas_padrao['accuracy'] > 0 else 0\n",
    "    reducao_fp = metricas_padrao['false_positives'] - metricas_isr['false_positives']\n",
    "    \n",
    "    print(f\"\\n[ANÁLISE]\")\n",
    "    print(f\"  Melhoria na Accuracy: {melhoria_accuracy:+.1f}%\")\n",
    "    print(f\"  Redução de False Positives: {reducao_fp} casos\")\n",
    "    \n",
    "    if metricas_isr['accuracy'] > metricas_padrao['accuracy']:\n",
    "        print(f\"\\n  [CONCLUSÃO] ISR MELHOROU o desempenho do modelo!\")\n",
    "    elif metricas_isr['accuracy'] == metricas_padrao['accuracy']:\n",
    "        print(f\"\\n  [CONCLUSÃO] ISR manteve o desempenho (sem impacto negativo)\")\n",
    "    else:\n",
    "        print(f\"\\n  [CONCLUSÃO] ISR reduziu accuracy (mas pode ter reduzido risco)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Análise por Tipo de Armadilha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*90)\n",
    "print(\"ANÁLISE POR TIPO DE ARMADILHA\")\n",
    "print(\"=\"*90)\n",
    "\n",
    "print(f\"\\n{'Tipo de Teste':<45} {'Padrão':>12} {'ISR':>12} {'Melhoria':>12}\")\n",
    "print(\"-\"*90)\n",
    "\n",
    "tipos = set(r[\"tipo_teste\"] for r in resultados_padrao)\n",
    "\n",
    "for tipo in sorted(tipos):\n",
    "    # Padrão\n",
    "    casos_padrao = [r for r in resultados_padrao if r[\"tipo_teste\"] == tipo]\n",
    "    acertos_padrao = sum(1 for r in casos_padrao if r[\"acertou\"])\n",
    "    total_tipo = len(casos_padrao)\n",
    "    taxa_padrao = acertos_padrao / total_tipo if total_tipo > 0 else 0\n",
    "    \n",
    "    # ISR\n",
    "    casos_isr = [r for r in resultados_isr if r[\"tipo_teste\"] == tipo]\n",
    "    acertos_isr = sum(1 for r in casos_isr if r[\"acertou\"])\n",
    "    taxa_isr = acertos_isr / total_tipo if total_tipo > 0 else 0\n",
    "    \n",
    "    # Melhoria\n",
    "    melhoria = \"✓\" if taxa_isr > taxa_padrao else (\"=\" if taxa_isr == taxa_padrao else \"✗\")\n",
    "    \n",
    "    print(f\"{tipo:<45} {taxa_padrao:>11.0%} {taxa_isr:>11.0%} {melhoria:>12}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "---\n",
    "## 11. Salvando Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": "output_dir = PROJECT_ROOT / \"outputs\" / \"metrics\"\noutput_dir.mkdir(parents=True, exist_ok=True)\n\ntimestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\noutput_file = output_dir / f\"comparacao_desafiador_isr_real_v5_{timestamp}.json\"\n\n# Limpar resultados para JSON\ndef limpar_para_json(obj):\n    \"\"\"Converte tipos numpy e bool para tipos nativos Python.\"\"\"\n    if isinstance(obj, dict):\n        return {k: limpar_para_json(v) for k, v in obj.items()}\n    elif isinstance(obj, list):\n        return [limpar_para_json(v) for v in obj]\n    elif isinstance(obj, (np.integer, np.int64, np.int32)):\n        return int(obj)\n    elif isinstance(obj, (np.floating, np.float64, np.float32)):\n        return float(obj)\n    elif isinstance(obj, np.ndarray):\n        return obj.tolist()\n    elif isinstance(obj, (np.bool_, bool)):\n        return bool(obj)\n    else:\n        return obj\n\ndef limpar_resultados(resultados):\n    limpos = []\n    for r in resultados:\n        item = {\n            \"cliente_id\": r[\"cliente_id\"],\n            \"tipo_teste\": r[\"tipo_teste\"],\n            \"esperado\": r[\"esperado\"],\n            \"obtido\": r[\"obtido\"],\n            \"acertou\": bool(r[\"acertou\"])\n        }\n        # Adicionar métricas ISR se existirem\n        if \"isr_acao\" in r:\n            item[\"isr_acao\"] = r[\"isr_acao\"]\n        if \"isr_metrics\" in r:\n            item[\"isr_metrics\"] = limpar_para_json(r[\"isr_metrics\"])\n        if \"isr_reason\" in r:\n            item[\"isr_reason\"] = r[\"isr_reason\"]\n        limpos.append(item)\n    return limpos\n\ndados_salvar = {\n    \"metadata\": {\n        \"timestamp\": timestamp,\n        \"modelo\": MODEL_NAME,\n        \"versao\": \"5.2-desafiador-isr-real\",\n        \"total_casos\": len(dataset),\n        \"isr_config\": {\n            \"target_confidence\": isr_auditor.target_confidence,\n            \"num_permutations\": isr_auditor.num_permutations,\n            \"clipping_b\": isr_auditor.clipping_b,\n            \"hard_veto_threshold\": isr_auditor.hard_veto_threshold,\n            \"prob_floor\": isr_auditor.PROB_FLOOR\n        }\n    },\n    \"metricas_padrao\": limpar_para_json(metricas_padrao),\n    \"metricas_isr\": limpar_para_json(metricas_isr),\n    \"resultados_padrao\": limpar_resultados(resultados_padrao),\n    \"resultados_isr\": limpar_resultados(resultados_isr)\n}\n\nwith open(output_file, \"w\", encoding=\"utf-8\") as f:\n    json.dump(dados_salvar, f, indent=2, ensure_ascii=False, default=str)\n\nprint(f\"[OK] Resultados salvos: {output_file}\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-25",
   "metadata": {},
   "source": "---\n## 12. Conclusões\n\n### ISR Real - Como funciona\n\nO ISR (Information Sufficiency Ratio) verifica se o modelo tem informação suficiente para a decisão:\n\n| Métrica ISR | Significado | Ação |\n|-------------|-------------|------|\n| **ISR >= 1.0** | Informação suficiente | APROVADO |\n| **ISR < 1.0** | Informação insuficiente | BLOQUEADO |\n| **ISR = 999** | Alta confiança (shortcut) | APROVADO imediato |\n| **Hard Veto** | Instabilidade severa | BLOQUEADO imediato |\n\n### Métricas de Classificação\n\n| Métrica | O que mede | Importância para Crédito |\n|---------|------------|-------------------------|\n| **Accuracy** | % de acertos totais | Visão geral |\n| **Precision** | Dos aprovados, quantos eram corretos | Evita aprovar indevidamente |\n| **Recall** | Dos que deveriam aprovar, quantos aprovou | Evita perder bons clientes |\n| **F1-Score** | Equilíbrio precision/recall | Métrica balanceada |\n| **False Positives** | Aprovações indevidas | **CRÍTICO para risco** |\n\n### Interpretação do ISR Real\n\nO ISR Real analisa a **consistência** da decisão do modelo:\n- Se o modelo é inconsistente entre permutações → ISR baixo → Bloqueio\n- Se o modelo é consistente e confiante → ISR alto → Aprovação\n\n### Diferença do ISR Real vs Regras Hard-coded\n\n| Aspecto | Regras Hard-coded | ISR Real |\n|---------|-------------------|----------|\n| **Verificação** | Lista fixa de regras | Análise de consistência |\n| **Adaptabilidade** | Rígido | Detecta padrões implícitos |\n| **Cobertura** | Apenas casos previstos | Qualquer caso com instabilidade |\n| **Custo** | Baixo (sem API) | Alto (múltiplas chamadas) |\n\n### Recomendações\n\n1. **ISR Real** é ideal para detectar incerteza do modelo, não apenas casos específicos\n2. O modelo pode \"acertar\" mas com baixa confiança - ISR captura isso\n3. Combine ISR com regras de negócio para cobertura completa"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}