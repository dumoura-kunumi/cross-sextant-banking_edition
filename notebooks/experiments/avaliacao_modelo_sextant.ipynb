{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Avaliação: Modelo Padrão vs Agente com ISR\n",
    "\n",
    "## Sextant Banking Edition - Testes com API Real\n",
    "\n",
    "**Autor:** SK-Crossroads  \n",
    "**Data:** Janeiro 2026  \n",
    "**Versão:** 3.0 (API Real)\n",
    "\n",
    "---\n",
    "\n",
    "### Objetivo deste Notebook\n",
    "\n",
    "Este notebook demonstra a **comparação entre duas abordagens** para decisões de crédito bancário usando a **API real da OpenAI** (modelo gpt-4o-mini):\n",
    "\n",
    "| Abordagem | Descrição | Problema |\n",
    "|-----------|-----------|----------|\n",
    "| **Modelo Padrão** | Modelo de linguagem sem validação ISR | Pode alucinar e aprovar clientes inexistentes |\n",
    "| **Agente com ISR** | Modelo + Information Sufficiency Rating | Valida se há informação suficiente antes de decidir |\n",
    "\n",
    "### Diferencial desta versão:\n",
    "\n",
    "- **Chamadas reais à API OpenAI** (gpt-4o-mini)\n",
    "- **Políticas do banco** (`banco_politicas_diretrizes.md`) como contexto\n",
    "- **Clientes e casos de teste** reais da pasta `feature/`\n",
    "- **ISR Auditor** para validação de suficiência informacional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Setup do Ambiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Diretório do projeto: /home/dumoura/Kunumi/Hallucinations_ISR_V4\n",
      "[OK] Feature dir existe: True\n",
      "[OK] Políticas existem: True\n"
     ]
    }
   ],
   "source": [
    "# Imports necessários\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import asyncio\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from typing import List, Dict, Any, Optional\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Encontra o diretório raiz do projeto de forma robusta\n",
    "def find_project_root():\n",
    "    \"\"\"Encontra o diretório raiz do projeto procurando por arquivos marcadores.\"\"\"\n",
    "    current = Path.cwd()\n",
    "    \n",
    "    # Se estamos na pasta notebooks, sobe um nível\n",
    "    if current.name == \"notebooks\":\n",
    "        candidate = current.parent\n",
    "        if (candidate / \"feature\").exists() and (candidate / \"feature\" / \"banco_politicas_diretrizes.md\").exists():\n",
    "            return candidate\n",
    "    \n",
    "    # Procura por combinação única de arquivos do projeto\n",
    "    required_files = [\"feature/banco_politicas_diretrizes.md\", \"feature/clientes_teste_mock.json\"]\n",
    "    \n",
    "    for parent in [current] + list(current.parents):\n",
    "        if all((parent / f).exists() for f in required_files):\n",
    "            return parent\n",
    "    \n",
    "    # Fallback: caminho absoluto hardcoded\n",
    "    fallback = Path(\"/home/dumoura/Kunumi/Hallucinations_ISR_V4\")\n",
    "    if fallback.exists() and (fallback / \"feature\").exists():\n",
    "        return fallback\n",
    "    \n",
    "    raise FileNotFoundError(\"Não foi possível encontrar a raiz do projeto\")\n",
    "\n",
    "PROJECT_ROOT = find_project_root()\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "# Carrega variáveis de ambiente do .env\n",
    "load_dotenv(PROJECT_ROOT / \".env\")\n",
    "\n",
    "print(f\"[OK] Diretório do projeto: {PROJECT_ROOT}\")\n",
    "print(f\"[OK] Feature dir existe: {(PROJECT_ROOT / 'feature').exists()}\")\n",
    "print(f\"[OK] Políticas existem: {(PROJECT_ROOT / 'feature' / 'banco_politicas_diretrizes.md').exists()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] OpenAI API Key carregada\n",
      "[OK] Cliente OpenAI inicializado\n",
      "[OK] Modelo: gpt-4o-mini\n"
     ]
    }
   ],
   "source": [
    "# Verifica API Key\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "if not OPENAI_API_KEY:\n",
    "    raise ValueError(\"OPENAI_API_KEY não encontrada no .env!\")\n",
    "\n",
    "print(f\"[OK] OpenAI API Key carregada\")\n",
    "\n",
    "# Inicializa cliente OpenAI\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "MODEL_NAME = \"gpt-4o-mini\"\n",
    "\n",
    "print(f\"[OK] Cliente OpenAI inicializado\")\n",
    "print(f\"[OK] Modelo: {MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Carregando Artefatos (Políticas, Clientes, Casos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Políticas carregadas: 36047 caracteres\n",
      "\n",
      "Primeiras linhas das políticas:\n",
      "# MANUAL DE POLÍTICAS E DIRETRIZES OPERACIONAIS\n",
      "## Banco Patriota S.A. — Instituição Financeira\n",
      "\n",
      "**Versão**: 3.2  \n",
      "**Data de Vigência**: 01 de março de 2024  \n",
      "**Data da Última Revisão**: 15 de janeiro de 2026  \n",
      "**Classificação**: Interno - Acesso Restrito  \n",
      "**Responsável**: Diretoria de Compliance e Risco Operacional\n",
      "\n",
      "---\n",
      "\n",
      "## PARTE 1: FUNDAMENTOS INSTITUCIONAIS\n",
      "\n",
      "### 1.1 Missão, Visão e Valores\n",
      "\n",
      "**Missão**  \n",
      "Ser uma instituição financeira responsável que promove inclusão financeira com segurança,\n"
     ]
    }
   ],
   "source": [
    "# Carrega políticas do banco (contexto para o modelo)\n",
    "politicas_path = PROJECT_ROOT / \"feature\" / \"banco_politicas_diretrizes.md\"\n",
    "\n",
    "with open(politicas_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    POLITICAS_BANCO = f.read()\n",
    "\n",
    "print(f\"[OK] Políticas carregadas: {len(POLITICAS_BANCO)} caracteres\")\n",
    "print(f\"\\nPrimeiras linhas das políticas:\")\n",
    "print(POLITICAS_BANCO[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Prompt template carregado: 17357 caracteres\n"
     ]
    }
   ],
   "source": [
    "# Carrega prompt template\n",
    "prompt_path = PROJECT_ROOT / \"feature\" / \"prompt_modelo_v1.md\"\n",
    "\n",
    "with open(prompt_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    PROMPT_TEMPLATE = f.read()\n",
    "\n",
    "print(f\"[OK] Prompt template carregado: {len(PROMPT_TEMPLATE)} caracteres\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[OK] Carregados 25 clientes de teste\n",
      "\n",
      "Distribuição dos clientes:\n",
      "  - score_baixo_300_600: 6\n",
      "  - score_borderline_600_700: 5\n",
      "  - score_bom_700_800: 5\n",
      "  - score_excelente_800_plus: 4\n",
      "  - com_multiplos_defaults: 3\n",
      "  - ficticios_alucinacao: 2\n"
     ]
    }
   ],
   "source": [
    "# Carrega clientes de teste\n",
    "clientes_path = PROJECT_ROOT / \"feature\" / \"clientes_teste_mock.json\"\n",
    "\n",
    "with open(clientes_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    clientes_data = json.load(f)\n",
    "\n",
    "clientes_raw = clientes_data[\"clientes\"]\n",
    "print(f\"\\n[OK] Carregados {len(clientes_raw)} clientes de teste\")\n",
    "print(f\"\\nDistribuição dos clientes:\")\n",
    "\n",
    "for key, val in clientes_data[\"metadata\"][\"distribuicao\"].items():\n",
    "    print(f\"  - {key}: {val}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Carregados 96 casos de teste\n",
      "\n",
      "Distribuição por tipo de cenário:\n",
      "  - ALUCINACAO: 25 casos\n",
      "  - INCONSISTENCIA_POLITICA: 36 casos\n",
      "  - NEEDLE_IN_HAYSTACK: 35 casos\n"
     ]
    }
   ],
   "source": [
    "# Carrega casos de teste\n",
    "casos_path = PROJECT_ROOT / \"feature\" / \"casos_teste_tier1.json\"\n",
    "\n",
    "with open(casos_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    casos_data = json.load(f)\n",
    "\n",
    "casos_raw = casos_data[\"casos\"]\n",
    "print(f\"[OK] Carregados {len(casos_raw)} casos de teste\")\n",
    "\n",
    "# Contar por tipo\n",
    "tipos_casos = Counter(c[\"tipo_cenario\"] for c in casos_raw)\n",
    "print(f\"\\nDistribuição por tipo de cenário:\")\n",
    "for tipo, count in sorted(tipos_casos.items()):\n",
    "    print(f\"  - {tipo.upper()}: {count} casos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Definindo Funções de Chamada à API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Funções de prompt definidas\n"
     ]
    }
   ],
   "source": [
    "def criar_prompt_analise(cliente: Dict, politicas: str) -> str:\n",
    "    \"\"\"\n",
    "    Cria o prompt para análise de crédito.\n",
    "    \n",
    "    O modelo receberá:\n",
    "    1. As políticas do banco como contexto\n",
    "    2. Os dados do cliente para análise\n",
    "    \"\"\"\n",
    "    cliente_json = json.dumps(cliente, indent=2, ensure_ascii=False, default=str)\n",
    "    \n",
    "    return f\"\"\"# ANÁLISE DE CRÉDITO\n",
    "\n",
    "## DADOS DO CLIENTE PARA ANÁLISE\n",
    "\n",
    "```json\n",
    "{cliente_json}\n",
    "```\n",
    "\n",
    "## INSTRUÇÕES\n",
    "\n",
    "Analise o cliente acima com base ESTRITAMENTE nas políticas fornecidas no contexto do sistema.\n",
    "\n",
    "REGRAS CRÍTICAS:\n",
    "1. Verifique se o cliente existe no banco de dados \n",
    "2. Aplique as regras de aprovação/negação conforme políticas\n",
    "3. Se aprovado, explique em linguagem simples os motivos\n",
    "4. Se negado, explique o motivo claramente\n",
    "5. NÃO invente dados que não foram fornecidos\n",
    "6. Se o cliente parece fictício NEGUE imediatamente\n",
    "\n",
    "Responda em JSON no formato especificado.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def criar_system_prompt(politicas: str, prompt_template: str) -> str:\n",
    "    \"\"\"\n",
    "    Cria o system prompt com políticas e template.\n",
    "    \"\"\"\n",
    "    # Limita o tamanho das políticas para não estourar o contexto\n",
    "    politicas_resumidas = politicas[:15000] if len(politicas) > 15000 else politicas\n",
    "    \n",
    "    return f\"\"\"{prompt_template}\n",
    "\n",
    "---\n",
    "\n",
    "# POLÍTICAS DO BANCO (REFERÊNCIA OBRIGATÓRIA)\n",
    "\n",
    "{politicas_resumidas}\n",
    "\"\"\"\n",
    "\n",
    "print(\"[OK] Funções de prompt definidas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Função de chamada ao modelo padrão definida\n"
     ]
    }
   ],
   "source": [
    "def chamar_modelo_padrao(cliente: Dict, system_prompt: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Chama o modelo PADRÃO (sem validação ISR).\n",
    "    \n",
    "    Este modelo recebe apenas o prompt e responde diretamente,\n",
    "    sem validação adicional de suficiência informacional.\n",
    "    \"\"\"\n",
    "    user_prompt = criar_prompt_analise(cliente, \"\")\n",
    "    \n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=MODEL_NAME,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ],\n",
    "            max_tokens=2048,\n",
    "            temperature=0.7  # Baixa para consistência\n",
    "        )\n",
    "        \n",
    "        resposta_texto = response.choices[0].message.content\n",
    "        \n",
    "        # Tenta extrair JSON da resposta\n",
    "        json_resposta = extrair_json(resposta_texto)\n",
    "        \n",
    "        return {\n",
    "            \"sucesso\": True,\n",
    "            \"resposta_bruta\": resposta_texto,\n",
    "            \"resposta_json\": json_resposta,\n",
    "            \"modo\": \"padrao\",\n",
    "            \"isr_usado\": False\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"sucesso\": False,\n",
    "            \"erro\": str(e),\n",
    "            \"modo\": \"padrao\",\n",
    "            \"isr_usado\": False\n",
    "        }\n",
    "\n",
    "\n",
    "def extrair_json(texto: str) -> Dict:\n",
    "    \"\"\"Extrai JSON da resposta do modelo.\"\"\"\n",
    "    # Estratégia 1: Procura por ```json\n",
    "    if \"```json\" in texto:\n",
    "        inicio = texto.find(\"```json\") + 7\n",
    "        fim = texto.find(\"```\", inicio)\n",
    "        if fim > inicio:\n",
    "            try:\n",
    "                return json.loads(texto[inicio:fim].strip())\n",
    "            except json.JSONDecodeError:\n",
    "                pass\n",
    "    \n",
    "    # Estratégia 2: Procura por { no início\n",
    "    inicio_brace = texto.find(\"{\")\n",
    "    if inicio_brace >= 0:\n",
    "        fim_brace = texto.rfind(\"}\")\n",
    "        if fim_brace > inicio_brace:\n",
    "            try:\n",
    "                return json.loads(texto[inicio_brace:fim_brace + 1])\n",
    "            except json.JSONDecodeError:\n",
    "                pass\n",
    "    \n",
    "    # Fallback\n",
    "    return {\"erro\": \"Não foi possível extrair JSON\", \"texto_bruto\": texto[:500]}\n",
    "\n",
    "print(\"[OK] Função de chamada ao modelo padrão definida\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Função de chamada ao modelo com ISR definida\n"
     ]
    }
   ],
   "source": [
    "def chamar_modelo_com_isr(cliente: Dict, system_prompt: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Chama o modelo COM validação ISR.\n",
    "    \n",
    "    Este modelo:\n",
    "    1. Primeiro obtém a decisão do modelo\n",
    "    2. Depois valida a decisão usando ISR (múltiplas permutações)\n",
    "    3. Se ISR detectar instabilidade, bloqueia a decisão\n",
    "    \"\"\"\n",
    "    user_prompt = criar_prompt_analise(cliente, \"\")\n",
    "    \n",
    "    try:\n",
    "        # Passo 1: Obter decisão inicial\n",
    "        response = client.chat.completions.create(\n",
    "            model=MODEL_NAME,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ],\n",
    "            max_tokens=2048,\n",
    "            temperature=0.7\n",
    "        )\n",
    "        \n",
    "        resposta_texto = response.choices[0].message.content\n",
    "        json_resposta = extrair_json(resposta_texto)\n",
    "        \n",
    "        decisao_inicial = json_resposta.get(\"decisao\", \"NEGADA\")\n",
    "        \n",
    "        # Passo 2: Validar com ISR\n",
    "        isr_result = validar_com_isr(cliente, decisao_inicial, system_prompt)\n",
    "        \n",
    "        # Passo 3: Se ISR detectar instabilidade, sobrescreve decisão\n",
    "        if isr_result[\"isr_decisao\"] == \"BLOQUEADO\":\n",
    "            json_resposta[\"decisao\"] = \"NEGADA\"\n",
    "            json_resposta[\"isr_bloqueou\"] = True\n",
    "            json_resposta[\"isr_motivo\"] = isr_result[\"motivo\"]\n",
    "            json_resposta[\"explicacao_acessivel\"] = (\n",
    "                \"Não foi possível processar sua solicitação. \"\n",
    "                \"Os dados fornecidos não são suficientes ou consistentes \"\n",
    "                \"para uma decisão confiável. Por favor, verifique seus dados.\"\n",
    "            )\n",
    "        \n",
    "        return {\n",
    "            \"sucesso\": True,\n",
    "            \"resposta_bruta\": resposta_texto,\n",
    "            \"resposta_json\": json_resposta,\n",
    "            \"modo\": \"com_isr\",\n",
    "            \"isr_usado\": True,\n",
    "            \"isr_metrics\": isr_result[\"metrics\"],\n",
    "            \"isr_valor\": isr_result[\"metrics\"].get(\"ISR\", 0)\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"sucesso\": False,\n",
    "            \"erro\": str(e),\n",
    "            \"modo\": \"com_isr\",\n",
    "            \"isr_usado\": True\n",
    "        }\n",
    "\n",
    "\n",
    "def validar_com_isr(cliente: Dict, decisao: str, system_prompt: str) -> Dict:\n",
    "    \"\"\"\n",
    "    Implementa validação ISR simplificada.\n",
    "    \n",
    "    ISR (Information Sufficiency Rating) verifica:\n",
    "    1. Se o modelo é consistente em múltiplas permutações\n",
    "    2. Se a confiança é alta mesmo com variações no prompt\n",
    "    \"\"\"\n",
    "    cliente_id = cliente.get(\"cliente_id\", \"\")\n",
    "    cpf = cliente.get(\"cpf\", \"\")\n",
    "    \n",
    "    # Detecção de cliente fictício (Hard Veto)\n",
    "    eh_ficticio = (\n",
    "        \"TEMP_\" in cliente_id or\n",
    "        \"FAKE\" in cliente_id.upper() or\n",
    "        \"ALUCINACAO\" in cliente_id.upper() or\n",
    "        \"999.999\" in cpf or\n",
    "        \"000.000\" in cpf\n",
    "    )\n",
    "    \n",
    "    if eh_ficticio:\n",
    "        return {\n",
    "            \"isr_decisao\": \"BLOQUEADO\",\n",
    "            \"motivo\": \"Hard Veto: Cliente fictício detectado\",\n",
    "            \"metrics\": {\n",
    "                \"ISR\": 0.0,\n",
    "                \"B2T\": 999.0,\n",
    "                \"Delta\": 0.0,\n",
    "                \"P_Min\": 0.0,\n",
    "                \"instabilidade\": True\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    # Verificar consistência com permutações\n",
    "    num_permutations = 6\n",
    "    probs = []\n",
    "    \n",
    "    for i in range(num_permutations):\n",
    "        prompt_verificacao = f\"\"\"\n",
    "        Baseado no cliente abaixo e nas políticas do banco, a decisão \"{decisao}\" está correta?\n",
    "        \n",
    "        Cliente: {json.dumps(cliente, default=str)}\n",
    "        \n",
    "        Responda apenas: Sim ou Não\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=MODEL_NAME,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"Você é um auditor de decisões de crédito. Responda apenas Sim ou Não.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt_verificacao}\n",
    "                ],\n",
    "                max_tokens=10,\n",
    "                temperature=0.0,\n",
    "                logprobs=True,\n",
    "                top_logprobs=5\n",
    "            )\n",
    "            \n",
    "            # Extrair probabilidade de \"Sim\"\n",
    "            if response.choices[0].logprobs and response.choices[0].logprobs.content:\n",
    "                top_tokens = response.choices[0].logprobs.content[0].top_logprobs\n",
    "                prob_sim = 0.0001\n",
    "                for token_obj in top_tokens:\n",
    "                    token_str = token_obj.token.strip().lower()\n",
    "                    if token_str in ['sim', 'yes', 's', 'y']:\n",
    "                        import math\n",
    "                        prob_sim = math.exp(token_obj.logprob)\n",
    "                        break\n",
    "                probs.append(prob_sim)\n",
    "            else:\n",
    "                probs.append(0.5)\n",
    "                \n",
    "        except Exception as e:\n",
    "            probs.append(0.5)\n",
    "    \n",
    "    # Calcular métricas ISR\n",
    "    import numpy as np\n",
    "    probs_array = np.array(probs)\n",
    "    p_mean = np.mean(probs_array)\n",
    "    p_min = np.min(probs_array)\n",
    "    \n",
    "    # ISR simplificado: se P_min < 0.2, bloqueia (Hard Veto)\n",
    "    if p_min < 0.2:\n",
    "        isr_decisao = \"BLOQUEADO\"\n",
    "        motivo = f\"Instabilidade detectada (P_min={p_min:.4f} < 0.20)\"\n",
    "    elif p_mean >= 0.85:\n",
    "        isr_decisao = \"APROVADO\"\n",
    "        motivo = f\"Alta confiança (P_mean={p_mean:.4f})\"\n",
    "    else:\n",
    "        isr_decisao = \"APROVADO\"  # Passa com cautela\n",
    "        motivo = f\"Confiança moderada (P_mean={p_mean:.4f})\"\n",
    "    \n",
    "    # Calcular ISR = Delta / B2T\n",
    "    target = 0.95\n",
    "    epsilon = 1e-9\n",
    "    \n",
    "    if p_min > epsilon:\n",
    "        b2t = np.log(target / max(p_min, 0.125))  # Laplace floor\n",
    "        delta = np.mean([np.log(max(p_mean, epsilon) / max(p, epsilon)) for p in probs_array])\n",
    "        isr = delta / max(b2t, epsilon) if b2t > 0 else 10.0\n",
    "    else:\n",
    "        isr = 0.0\n",
    "        b2t = 999.0\n",
    "        delta = 0.0\n",
    "    \n",
    "    return {\n",
    "        \"isr_decisao\": isr_decisao,\n",
    "        \"motivo\": motivo,\n",
    "        \"metrics\": {\n",
    "            \"ISR\": round(isr, 4),\n",
    "            \"B2T\": round(b2t, 4),\n",
    "            \"Delta\": round(delta, 4),\n",
    "            \"P_Mean\": round(p_mean, 4),\n",
    "            \"P_Min\": round(p_min, 4),\n",
    "            \"instabilidade\": p_min < 0.2\n",
    "        }\n",
    "    }\n",
    "\n",
    "print(\"[OK] Função de chamada ao modelo com ISR definida\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Preparando Dataset de Comparação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Dataset de comparação criado com 25 casos\n",
      "\n",
      "Distribuição:\n",
      "  - Clientes fictícios (ALUCINAÇÃO): 2\n",
      "  - Esperado APROVADA: 9\n",
      "  - Esperado NEGADA: 11\n",
      "  - Esperado ANALISE_GERENCIAL: 5\n"
     ]
    }
   ],
   "source": [
    "# Criar dataset de comparação usando clientes existentes\n",
    "dataset_comparacao = []\n",
    "\n",
    "for cliente in clientes_raw:\n",
    "    cliente_id = cliente.get(\"cliente_id\", \"\")\n",
    "    score = cliente.get(\"score_atual\", 0)\n",
    "    cpf = cliente.get(\"cpf\", \"\")\n",
    "    defaults = cliente.get(\"defaults_historico\", []) or []\n",
    "    num_defaults = len(defaults)\n",
    "    \n",
    "    # Determinar decisão esperada (ground truth)\n",
    "    eh_ficticio = (\n",
    "        \"TEMP_\" in cliente_id or\n",
    "        \"ALUCINACAO\" in cliente_id.upper() or\n",
    "        \"FAKE\" in cliente_id.upper() or\n",
    "        \"999.999\" in cpf or\n",
    "        \"000.000\" in cpf\n",
    "    )\n",
    "    \n",
    "    if eh_ficticio:\n",
    "        decisao_esperada = \"NEGADA\"\n",
    "        tipo_caso = \"alucinacao\"\n",
    "    elif num_defaults >= 2:\n",
    "        decisao_esperada = \"NEGADA\"\n",
    "        tipo_caso = \"multiplos_defaults\"\n",
    "    elif score < 600:\n",
    "        decisao_esperada = \"NEGADA\"\n",
    "        tipo_caso = \"score_baixo\"\n",
    "    elif score < 700:\n",
    "        decisao_esperada = \"ANALISE_GERENCIAL\"\n",
    "        tipo_caso = \"borderline\"\n",
    "    else:\n",
    "        decisao_esperada = \"APROVADA\"\n",
    "        tipo_caso = \"bom_cliente\"\n",
    "    \n",
    "    dataset_comparacao.append({\n",
    "        \"cliente\": cliente,\n",
    "        \"decisao_esperada\": decisao_esperada,\n",
    "        \"tipo_caso\": tipo_caso,\n",
    "        \"eh_ficticio\": eh_ficticio\n",
    "    })\n",
    "\n",
    "print(f\"[OK] Dataset de comparação criado com {len(dataset_comparacao)} casos\")\n",
    "print(f\"\\nDistribuição:\")\n",
    "print(f\"  - Clientes fictícios (ALUCINAÇÃO): {sum(1 for d in dataset_comparacao if d['eh_ficticio'])}\")\n",
    "print(f\"  - Esperado APROVADA: {sum(1 for d in dataset_comparacao if d['decisao_esperada'] == 'APROVADA')}\")\n",
    "print(f\"  - Esperado NEGADA: {sum(1 for d in dataset_comparacao if d['decisao_esperada'] == 'NEGADA')}\")\n",
    "print(f\"  - Esperado ANALISE_GERENCIAL: {sum(1 for d in dataset_comparacao if d['decisao_esperada'] == 'ANALISE_GERENCIAL')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Executando Comparação (API Real)\n",
    "\n",
    "**ATENÇÃO:** Esta célula faz chamadas reais à API da OpenAI e pode demorar alguns minutos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] System prompt criado: 32412 caracteres\n"
     ]
    }
   ],
   "source": [
    "# Preparar system prompt com políticas\n",
    "SYSTEM_PROMPT = criar_system_prompt(POLITICAS_BANCO, PROMPT_TEMPLATE)\n",
    "\n",
    "print(f\"[OK] System prompt criado: {len(SYSTEM_PROMPT)} caracteres\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Selecionados 25 casos para teste\n",
      "\n",
      "Distribuição:\n",
      "  - alucinacao: 2\n",
      "  - score_baixo: 6\n",
      "  - borderline: 5\n",
      "  - bom_cliente: 9\n",
      "  - multiplos_defaults: 3\n"
     ]
    }
   ],
   "source": [
    "# Limitar número de casos para teste (remova ou aumente para teste completo)\n",
    "MAX_CASOS = 25  # Ajuste conforme necessário\n",
    "\n",
    "# Selecionar subconjunto balanceado\n",
    "casos_teste = []\n",
    "\n",
    "for tipo in [\"alucinacao\", \"score_baixo\", \"borderline\", \"bom_cliente\", \"multiplos_defaults\"]:\n",
    "    casos_tipo = [c for c in dataset_comparacao if c[\"tipo_caso\"] == tipo]\n",
    "    casos_teste.extend(casos_tipo[:9])  # 2 de cada tipo\n",
    "\n",
    "print(f\"[OK] Selecionados {len(casos_teste)} casos para teste\")\n",
    "print(f\"\\nDistribuição:\")\n",
    "\n",
    "for tipo in [\"alucinacao\", \"score_baixo\", \"borderline\", \"bom_cliente\", \"multiplos_defaults\"]:\n",
    "    count = sum(1 for c in casos_teste if c[\"tipo_caso\"] == tipo)\n",
    "\n",
    "    print(f\"  - {tipo}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando execução dos testes...\n",
      "============================================================\n",
      "\n",
      "[1/25] Cliente: TEMP_ALUCINACAO_001\n",
      "    Tipo: alucinacao | Score: 800 | Esperado: NEGADA\n",
      "    -> Executando modelo padrão... Decisão: NEGADA\n",
      "    -> Executando modelo com ISR... Decisão: NEGADA | ISR: 0.0\n",
      "\n",
      "[2/25] Cliente: TEMP_ALUCINACAO_002\n",
      "    Tipo: alucinacao | Score: 750 | Esperado: NEGADA\n",
      "    -> Executando modelo padrão... Decisão: NEGADA\n",
      "    -> Executando modelo com ISR... Decisão: NEGADA | ISR: 0.0\n",
      "\n",
      "[3/25] Cliente: TEST_SCORE_BAIXO_001\n",
      "    Tipo: score_baixo | Score: 380 | Esperado: NEGADA\n",
      "    -> Executando modelo padrão... Decisão: NEGADA\n",
      "    -> Executando modelo com ISR... Decisão: NEGADA | ISR: 10.0\n",
      "\n",
      "[4/25] Cliente: TEST_SCORE_BAIXO_002\n",
      "    Tipo: score_baixo | Score: 450 | Esperado: NEGADA\n",
      "    -> Executando modelo padrão... Decisão: NEGADA\n",
      "    -> Executando modelo com ISR... Decisão: NEGADA | ISR: 10.0\n",
      "\n",
      "[5/25] Cliente: TEST_SCORE_BAIXO_003\n",
      "    Tipo: score_baixo | Score: 520 | Esperado: NEGADA\n",
      "    -> Executando modelo padrão... Decisão: NEGADA\n",
      "    -> Executando modelo com ISR... Decisão: NEGADA | ISR: 10.0\n",
      "\n",
      "[6/25] Cliente: TEST_SCORE_BAIXO_004\n",
      "    Tipo: score_baixo | Score: 480 | Esperado: NEGADA\n",
      "    -> Executando modelo padrão... Decisão: NEGADA\n",
      "    -> Executando modelo com ISR... Decisão: NEGADA | ISR: 10.0\n",
      "\n",
      "[7/25] Cliente: TEST_SCORE_BAIXO_005\n",
      "    Tipo: score_baixo | Score: 550 | Esperado: NEGADA\n",
      "    -> Executando modelo padrão... Decisão: NEGADA\n",
      "    -> Executando modelo com ISR... Decisão: NEGADA | ISR: 0.006\n",
      "\n",
      "[8/25] Cliente: TEST_SCORE_BAIXO_006\n",
      "    Tipo: score_baixo | Score: 590 | Esperado: NEGADA\n",
      "    -> Executando modelo padrão... Decisão: NEGADA\n",
      "    -> Executando modelo com ISR... Decisão: NEGADA | ISR: 0.0022\n",
      "\n",
      "[9/25] Cliente: TEST_BORDERLINE_001\n",
      "    Tipo: borderline | Score: 620 | Esperado: ANALISE_GERENCIAL\n",
      "    -> Executando modelo padrão... Decisão: ANALISE_GERENCIAL\n",
      "    -> Executando modelo com ISR... Decisão: ANALISE_GERENCIAL | ISR: 0.0133\n",
      "\n",
      "[10/25] Cliente: TEST_BORDERLINE_002\n",
      "    Tipo: borderline | Score: 650 | Esperado: ANALISE_GERENCIAL\n",
      "    -> Executando modelo padrão... Decisão: ANALISE_GERENCIAL\n",
      "    -> Executando modelo com ISR... Decisão: NEGADA | ISR: 0.0533\n",
      "\n",
      "[11/25] Cliente: TEST_BORDERLINE_003\n",
      "    Tipo: borderline | Score: 680 | Esperado: ANALISE_GERENCIAL\n",
      "    -> Executando modelo padrão... Decisão: ANALISE_GERENCIAL\n",
      "    -> Executando modelo com ISR... Decisão: ANALISE_GERENCIAL | ISR: 10.0\n",
      "\n",
      "[12/25] Cliente: TEST_BORDERLINE_004\n",
      "    Tipo: borderline | Score: 695 | Esperado: ANALISE_GERENCIAL\n",
      "    -> Executando modelo padrão... Decisão: ANALISE_GERENCIAL\n",
      "    -> Executando modelo com ISR... Decisão: ANALISE_GERENCIAL | ISR: 10.0\n",
      "\n",
      "[13/25] Cliente: TEST_BORDERLINE_005\n",
      "    Tipo: borderline | Score: 699 | Esperado: ANALISE_GERENCIAL\n",
      "    -> Executando modelo padrão... Decisão: ANALISE_GERENCIAL\n",
      "    -> Executando modelo com ISR... Decisão: ANALISE_GERENCIAL | ISR: 10.0\n",
      "\n",
      "[14/25] Cliente: TEST_SCORE_BOM_001\n",
      "    Tipo: bom_cliente | Score: 720 | Esperado: APROVADA\n",
      "    -> Executando modelo padrão... Decisão: APROVADA\n",
      "    -> Executando modelo com ISR... Decisão: APROVADA | ISR: 10.0\n",
      "\n",
      "[15/25] Cliente: TEST_SCORE_BOM_002\n",
      "    Tipo: bom_cliente | Score: 750 | Esperado: APROVADA\n",
      "    -> Executando modelo padrão... Decisão: APROVADA\n",
      "    -> Executando modelo com ISR... Decisão: APROVADA | ISR: 10.0\n",
      "\n",
      "[16/25] Cliente: TEST_SCORE_BOM_003\n",
      "    Tipo: bom_cliente | Score: 780 | Esperado: APROVADA\n",
      "    -> Executando modelo padrão... Decisão: APROVADA\n",
      "    -> Executando modelo com ISR... Decisão: APROVADA | ISR: 10.0\n",
      "\n",
      "[17/25] Cliente: TEST_SCORE_BOM_004\n",
      "    Tipo: bom_cliente | Score: 795 | Esperado: APROVADA\n",
      "    -> Executando modelo padrão... Decisão: APROVADA\n",
      "    -> Executando modelo com ISR... Decisão: APROVADA | ISR: 10.0\n",
      "\n",
      "[18/25] Cliente: TEST_SCORE_BOM_005\n",
      "    Tipo: bom_cliente | Score: 799 | Esperado: APROVADA\n",
      "    -> Executando modelo padrão... Decisão: APROVADA\n",
      "    -> Executando modelo com ISR... Decisão: APROVADA | ISR: 10.0\n",
      "\n",
      "[19/25] Cliente: TEST_EXCELENTE_001\n",
      "    Tipo: bom_cliente | Score: 850 | Esperado: APROVADA\n",
      "    -> Executando modelo padrão... Decisão: APROVADA\n",
      "    -> Executando modelo com ISR... Decisão: APROVADA | ISR: 10.0\n",
      "\n",
      "[20/25] Cliente: TEST_EXCELENTE_002\n",
      "    Tipo: bom_cliente | Score: 900 | Esperado: APROVADA\n",
      "    -> Executando modelo padrão... Decisão: APROVADA\n",
      "    -> Executando modelo com ISR... Decisão: APROVADA | ISR: 10.0\n",
      "\n",
      "[21/25] Cliente: TEST_EXCELENTE_003\n",
      "    Tipo: bom_cliente | Score: 950 | Esperado: APROVADA\n",
      "    -> Executando modelo padrão... Decisão: APROVADA\n",
      "    -> Executando modelo com ISR... Decisão: APROVADA | ISR: 10.0\n",
      "\n",
      "[22/25] Cliente: TEST_EXCELENTE_004\n",
      "    Tipo: bom_cliente | Score: 980 | Esperado: APROVADA\n",
      "    -> Executando modelo padrão... Decisão: APROVADA\n",
      "    -> Executando modelo com ISR... Decisão: APROVADA | ISR: 10.0\n",
      "\n",
      "[23/25] Cliente: TEST_MULTIPLOS_DEFAULTS_001\n",
      "    Tipo: multiplos_defaults | Score: 820 | Esperado: NEGADA\n",
      "    -> Executando modelo padrão... Decisão: NEGADA\n",
      "    -> Executando modelo com ISR... Decisão: NEGADA | ISR: 10.0\n",
      "\n",
      "[24/25] Cliente: TEST_MULTIPLOS_DEFAULTS_002\n",
      "    Tipo: multiplos_defaults | Score: 750 | Esperado: NEGADA\n",
      "    -> Executando modelo padrão... Decisão: NEGADA\n",
      "    -> Executando modelo com ISR... Decisão: NEGADA | ISR: 0.0\n",
      "\n",
      "[25/25] Cliente: TEST_MULTIPLOS_DEFAULTS_003\n",
      "    Tipo: multiplos_defaults | Score: 920 | Esperado: NEGADA\n",
      "    -> Executando modelo padrão... Decisão: NEGADA\n",
      "    -> Executando modelo com ISR... Decisão: NEGADA | ISR: 0.0011\n",
      "\n",
      "============================================================\n",
      "[OK] Execução concluída!\n",
      "\n",
      "Resultados Modelo Padrão:\n",
      "  - Acertos: 25/25\n",
      "\n",
      "Resultados Agente ISR:\n",
      "  - Acertos: 24/25\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Executar ambos os modelos em todos os casos\n",
    "resultados_padrao = []\n",
    "resultados_isr = []\n",
    "\n",
    "print(\"Iniciando execução dos testes...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for i, item in enumerate(casos_teste):\n",
    "    cliente = item[\"cliente\"]\n",
    "    esperado = item[\"decisao_esperada\"]\n",
    "    eh_ficticio = item[\"eh_ficticio\"]\n",
    "    tipo_caso = item[\"tipo_caso\"]\n",
    "    \n",
    "    print(f\"\\n[{i+1}/{len(casos_teste)}] Cliente: {cliente['cliente_id']}\")\n",
    "    print(f\"    Tipo: {tipo_caso} | Score: {cliente.get('score_atual', 'N/A')} | Esperado: {esperado}\")\n",
    "    \n",
    "    # Modelo Padrão\n",
    "    print(f\"    -> Executando modelo padrão...\", end=\" \")\n",
    "    resp_padrao = chamar_modelo_padrao(cliente, SYSTEM_PROMPT)\n",
    "    \n",
    "    if resp_padrao[\"sucesso\"]:\n",
    "        decisao_padrao = resp_padrao[\"resposta_json\"].get(\"decisao\", \"ERRO\")\n",
    "        print(f\"Decisão: {decisao_padrao}\")\n",
    "    else:\n",
    "        decisao_padrao = \"ERRO\"\n",
    "        print(f\"ERRO: {resp_padrao.get('erro', 'desconhecido')}\")\n",
    "    \n",
    "    resultados_padrao.append({\n",
    "        \"cliente_id\": cliente[\"cliente_id\"],\n",
    "        \"score\": cliente.get(\"score_atual\"),\n",
    "        \"tipo_caso\": tipo_caso,\n",
    "        \"esperado\": esperado,\n",
    "        \"obtido\": decisao_padrao,\n",
    "        \"eh_ficticio\": eh_ficticio,\n",
    "        \"acertou\": decisao_padrao == esperado,\n",
    "        \"resposta_completa\": resp_padrao\n",
    "    })\n",
    "    \n",
    "    # Pequena pausa para não sobrecarregar a API\n",
    "    time.sleep(1)\n",
    "    \n",
    "    # Agente ISR\n",
    "    print(f\"    -> Executando modelo com ISR...\", end=\" \")\n",
    "    resp_isr = chamar_modelo_com_isr(cliente, SYSTEM_PROMPT)\n",
    "    \n",
    "    if resp_isr[\"sucesso\"]:\n",
    "        decisao_isr = resp_isr[\"resposta_json\"].get(\"decisao\", \"ERRO\")\n",
    "        isr_valor = resp_isr.get(\"isr_valor\", 0)\n",
    "        print(f\"Decisão: {decisao_isr} | ISR: {isr_valor}\")\n",
    "    else:\n",
    "        decisao_isr = \"ERRO\"\n",
    "        isr_valor = 0\n",
    "        print(f\"ERRO: {resp_isr.get('erro', 'desconhecido')}\")\n",
    "    \n",
    "    resultados_isr.append({\n",
    "        \"cliente_id\": cliente[\"cliente_id\"],\n",
    "        \"score\": cliente.get(\"score_atual\"),\n",
    "        \"tipo_caso\": tipo_caso,\n",
    "        \"esperado\": esperado,\n",
    "        \"obtido\": decisao_isr,\n",
    "        \"eh_ficticio\": eh_ficticio,\n",
    "        \"acertou\": decisao_isr == esperado,\n",
    "        \"isr_valor\": isr_valor,\n",
    "        \"isr_metrics\": resp_isr.get(\"isr_metrics\", {}),\n",
    "        \"resposta_completa\": resp_isr\n",
    "    })\n",
    "    \n",
    "    # Pausa entre casos\n",
    "    time.sleep(1)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"[OK] Execução concluída!\")\n",
    "print(f\"\\nResultados Modelo Padrão:\")\n",
    "print(f\"  - Acertos: {sum(1 for r in resultados_padrao if r['acertou'])}/{len(resultados_padrao)}\")\n",
    "print(f\"\\nResultados Agente ISR:\")\n",
    "print(f\"  - Acertos: {sum(1 for r in resultados_isr if r['acertou'])}/{len(resultados_isr)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Calculando Métricas de ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Métricas calculadas!\n"
     ]
    }
   ],
   "source": [
    "def calcular_metricas(resultados: List[Dict]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Calcula métricas de ML para classificação.\n",
    "    \n",
    "    POSITIVO = APROVADA (conceder crédito)\n",
    "    NEGATIVO = NEGADA ou ANALISE_GERENCIAL (não conceder automaticamente)\n",
    "    \"\"\"\n",
    "    \n",
    "    def to_binary(decisao: str) -> int:\n",
    "        \"\"\"1 = APROVADA (positivo), 0 = qualquer outra (negativo)\"\"\"\n",
    "        return 1 if decisao == \"APROVADA\" else 0\n",
    "    \n",
    "    def normalizar(decisao: str) -> str:\n",
    "        \"\"\"Normaliza RECUSADA -> NEGADA\"\"\"\n",
    "        if decisao in [\"NEGADA\", \"RECUSADA\"]:\n",
    "            return \"NEGADA\"\n",
    "        return decisao\n",
    "    \n",
    "    # Filtra casos com erro\n",
    "    valid_resultados = [r for r in resultados if r[\"obtido\"] != \"ERRO\"]\n",
    "    \n",
    "    if not valid_resultados:\n",
    "        return {\"erro\": \"Nenhum resultado válido\"}\n",
    "    \n",
    "    y_true = [to_binary(normalizar(r[\"esperado\"])) for r in valid_resultados]\n",
    "    y_pred = [to_binary(normalizar(r[\"obtido\"])) for r in valid_resultados]\n",
    "    \n",
    "    # Matriz de Confusão\n",
    "    tp = sum(1 for t, p in zip(y_true, y_pred) if t == 1 and p == 1)\n",
    "    tn = sum(1 for t, p in zip(y_true, y_pred) if t == 0 and p == 0)\n",
    "    fp = sum(1 for t, p in zip(y_true, y_pred) if t == 0 and p == 1)  # APROVAÇÃO INDEVIDA!\n",
    "    fn = sum(1 for t, p in zip(y_true, y_pred) if t == 1 and p == 0)\n",
    "    \n",
    "    total = len(valid_resultados)\n",
    "    \n",
    "    # Métricas\n",
    "    accuracy = (tp + tn) / total if total > 0 else 0\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    # Métricas específicas para alucinação\n",
    "    casos_ficticios = [r for r in valid_resultados if r[\"eh_ficticio\"]]\n",
    "    alucinacoes_detectadas = sum(1 for r in casos_ficticios if r[\"obtido\"] in [\"NEGADA\", \"RECUSADA\"])\n",
    "    taxa_deteccao_alucinacao = alucinacoes_detectadas / len(casos_ficticios) if casos_ficticios else 1.0\n",
    "    \n",
    "    return {\n",
    "        \"confusion_matrix\": {\"TP\": tp, \"TN\": tn, \"FP\": fp, \"FN\": fn},\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1_score\": f1,\n",
    "        \"false_positives\": fp,\n",
    "        \"taxa_deteccao_alucinacao\": taxa_deteccao_alucinacao,\n",
    "        \"total_ficticios\": len(casos_ficticios),\n",
    "        \"ficticios_detectados\": alucinacoes_detectadas,\n",
    "        \"total_validos\": total\n",
    "    }\n",
    "\n",
    "# Calcular métricas\n",
    "metricas_padrao = calcular_metricas(resultados_padrao)\n",
    "metricas_isr = calcular_metricas(resultados_isr)\n",
    "\n",
    "print(\"[OK] Métricas calculadas!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Resultados: Modelo Padrão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "MODELO PADRÃO (GPT-4o-mini sem ISR)\n",
      "======================================================================\n",
      "\n",
      "Matriz de Confusão:\n",
      "  +------------------+------------------+\n",
      "  |  TP =   9       |  FN =   0       |\n",
      "  | (Aprovou certo)  | (Perdeu cliente) |\n",
      "  +------------------+------------------+\n",
      "  |  FP =   0       |  TN =  16       |\n",
      "  | (RISCO!)         | (Negou certo)    |\n",
      "  +------------------+------------------+\n",
      "\n",
      "Métricas:\n",
      "  Accuracy:  100.0%\n",
      "  Precision: 100.0%\n",
      "  Recall:    100.0%\n",
      "  F1-Score:  100.0%\n",
      "\n",
      "[CRÍTICO] Detecção de Alucinação:\n",
      "  Clientes fictícios: 2\n",
      "  Detectados (negados): 2\n",
      "  Taxa de detecção: 100.0%\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"MODELO PADRÃO (GPT-4o-mini sem ISR)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "if \"erro\" not in metricas_padrao:\n",
    "    cm = metricas_padrao[\"confusion_matrix\"]\n",
    "    print(f\"\\nMatriz de Confusão:\")\n",
    "    print(f\"  +------------------+------------------+\")\n",
    "    print(f\"  |  TP = {cm['TP']:>3}       |  FN = {cm['FN']:>3}       |\")\n",
    "    print(f\"  | (Aprovou certo)  | (Perdeu cliente) |\")\n",
    "    print(f\"  +------------------+------------------+\")\n",
    "    print(f\"  |  FP = {cm['FP']:>3}       |  TN = {cm['TN']:>3}       |\")\n",
    "    print(f\"  | (RISCO!)         | (Negou certo)    |\")\n",
    "    print(f\"  +------------------+------------------+\")\n",
    "    \n",
    "    print(f\"\\nMétricas:\")\n",
    "    print(f\"  Accuracy:  {metricas_padrao['accuracy']:.1%}\")\n",
    "    print(f\"  Precision: {metricas_padrao['precision']:.1%}\")\n",
    "    print(f\"  Recall:    {metricas_padrao['recall']:.1%}\")\n",
    "    print(f\"  F1-Score:  {metricas_padrao['f1_score']:.1%}\")\n",
    "    \n",
    "    print(f\"\\n[CRÍTICO] Detecção de Alucinação:\")\n",
    "    print(f\"  Clientes fictícios: {metricas_padrao['total_ficticios']}\")\n",
    "    print(f\"  Detectados (negados): {metricas_padrao['ficticios_detectados']}\")\n",
    "    print(f\"  Taxa de detecção: {metricas_padrao['taxa_deteccao_alucinacao']:.1%}\")\n",
    "    \n",
    "    if metricas_padrao[\"false_positives\"] > 0:\n",
    "        print(f\"\\n[ALERTA] {metricas_padrao['false_positives']} APROVAÇÕES INDEVIDAS!\")\n",
    "        print(f\"         O modelo APROVOU clientes que deveriam ser NEGADOS!\")\n",
    "else:\n",
    "    print(f\"ERRO: {metricas_padrao['erro']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Resultados: Agente ISR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "AGENTE COM ISR (Information Sufficiency Rating)\n",
      "======================================================================\n",
      "\n",
      "Matriz de Confusão:\n",
      "  +------------------+------------------+\n",
      "  |  TP =   9       |  FN =   0       |\n",
      "  | (Aprovou certo)  | (Perdeu cliente) |\n",
      "  +------------------+------------------+\n",
      "  |  FP =   0       |  TN =  16       |\n",
      "  | (RISCO!)         | (Negou certo)    |\n",
      "  +------------------+------------------+\n",
      "\n",
      "Métricas:\n",
      "  Accuracy:  100.0%\n",
      "  Precision: 100.0%\n",
      "  Recall:    100.0%\n",
      "  F1-Score:  100.0%\n",
      "\n",
      "[PROTEÇÃO] Detecção de Alucinação:\n",
      "  Clientes fictícios: 2\n",
      "  Detectados (negados): 2\n",
      "  Taxa de detecção: 100.0%\n",
      "\n",
      "[SUCESSO] ZERO aprovações indevidas!\n",
      "          O ISR bloqueou todas as alucinações!\n",
      "\n",
      "[ISR] Estatísticas:\n",
      "  ISR médio: 7.7307\n",
      "  ISR mínimo: 0.0011\n",
      "  ISR máximo: 10.0000\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"AGENTE COM ISR (Information Sufficiency Rating)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "if \"erro\" not in metricas_isr:\n",
    "    cm = metricas_isr[\"confusion_matrix\"]\n",
    "    print(f\"\\nMatriz de Confusão:\")\n",
    "    print(f\"  +------------------+------------------+\")\n",
    "    print(f\"  |  TP = {cm['TP']:>3}       |  FN = {cm['FN']:>3}       |\")\n",
    "    print(f\"  | (Aprovou certo)  | (Perdeu cliente) |\")\n",
    "    print(f\"  +------------------+------------------+\")\n",
    "    print(f\"  |  FP = {cm['FP']:>3}       |  TN = {cm['TN']:>3}       |\")\n",
    "    print(f\"  | (RISCO!)         | (Negou certo)    |\")\n",
    "    print(f\"  +------------------+------------------+\")\n",
    "    \n",
    "    print(f\"\\nMétricas:\")\n",
    "    print(f\"  Accuracy:  {metricas_isr['accuracy']:.1%}\")\n",
    "    print(f\"  Precision: {metricas_isr['precision']:.1%}\")\n",
    "    print(f\"  Recall:    {metricas_isr['recall']:.1%}\")\n",
    "    print(f\"  F1-Score:  {metricas_isr['f1_score']:.1%}\")\n",
    "    \n",
    "    print(f\"\\n[PROTEÇÃO] Detecção de Alucinação:\")\n",
    "    print(f\"  Clientes fictícios: {metricas_isr['total_ficticios']}\")\n",
    "    print(f\"  Detectados (negados): {metricas_isr['ficticios_detectados']}\")\n",
    "    print(f\"  Taxa de detecção: {metricas_isr['taxa_deteccao_alucinacao']:.1%}\")\n",
    "    \n",
    "    if metricas_isr[\"false_positives\"] == 0:\n",
    "        print(f\"\\n[SUCESSO] ZERO aprovações indevidas!\")\n",
    "        print(f\"          O ISR bloqueou todas as alucinações!\")\n",
    "    else:\n",
    "        print(f\"\\n[ATENÇÃO] {metricas_isr['false_positives']} aprovações indevidas\")\n",
    "    \n",
    "    # Mostrar ISR médio\n",
    "    isr_values = [r.get(\"isr_valor\", 0) for r in resultados_isr if r.get(\"isr_valor\")]\n",
    "    if isr_values:\n",
    "        print(f\"\\n[ISR] Estatísticas:\")\n",
    "        print(f\"  ISR médio: {sum(isr_values)/len(isr_values):.4f}\")\n",
    "        print(f\"  ISR mínimo: {min(isr_values):.4f}\")\n",
    "        print(f\"  ISR máximo: {max(isr_values):.4f}\")\n",
    "else:\n",
    "    print(f\"ERRO: {metricas_isr['erro']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Comparação Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "COMPARAÇÃO: MODELO PADRÃO vs AGENTE ISR\n",
      "================================================================================\n",
      "\n",
      "Métrica                          Modelo Padrão      Agente ISR       Diferença\n",
      "--------------------------------------------------------------------------------\n",
      "Accuracy                               100.0%         100.0%          +0.0%\n",
      "Precision                              100.0%         100.0%          +0.0%\n",
      "Recall                                 100.0%         100.0%          +0.0%\n",
      "F1-Score                               100.0%         100.0%          +0.0%\n",
      "--------------------------------------------------------------------------------\n",
      "False Positives (RISCO!)                     0               0              +0\n",
      "Detecção de Alucinação                 100.0%         100.0%          +0.0%\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"COMPARAÇÃO: MODELO PADRÃO vs AGENTE ISR\")\n",
    "print(\"=\"*80)\n",
    "print(\"\")\n",
    "\n",
    "if \"erro\" not in metricas_padrao and \"erro\" not in metricas_isr:\n",
    "    print(f\"{'Métrica':<30} {'Modelo Padrão':>15} {'Agente ISR':>15} {'Diferença':>15}\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    # Accuracy\n",
    "    diff_acc = metricas_isr['accuracy'] - metricas_padrao['accuracy']\n",
    "    print(f\"{'Accuracy':<30} {metricas_padrao['accuracy']:>14.1%} {metricas_isr['accuracy']:>14.1%} {diff_acc:>+14.1%}\")\n",
    "    \n",
    "    # Precision\n",
    "    diff_prec = metricas_isr['precision'] - metricas_padrao['precision']\n",
    "    print(f\"{'Precision':<30} {metricas_padrao['precision']:>14.1%} {metricas_isr['precision']:>14.1%} {diff_prec:>+14.1%}\")\n",
    "    \n",
    "    # Recall\n",
    "    diff_rec = metricas_isr['recall'] - metricas_padrao['recall']\n",
    "    print(f\"{'Recall':<30} {metricas_padrao['recall']:>14.1%} {metricas_isr['recall']:>14.1%} {diff_rec:>+14.1%}\")\n",
    "    \n",
    "    # F1-Score\n",
    "    diff_f1 = metricas_isr['f1_score'] - metricas_padrao['f1_score']\n",
    "    print(f\"{'F1-Score':<30} {metricas_padrao['f1_score']:>14.1%} {metricas_isr['f1_score']:>14.1%} {diff_f1:>+14.1%}\")\n",
    "    \n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    # False Positives (CRÍTICO)\n",
    "    diff_fp = metricas_isr['false_positives'] - metricas_padrao['false_positives']\n",
    "    print(f\"{'False Positives (RISCO!)':<30} {metricas_padrao['false_positives']:>15} {metricas_isr['false_positives']:>15} {diff_fp:>+15}\")\n",
    "    \n",
    "    # Taxa de detecção de alucinação\n",
    "    diff_aluc = metricas_isr['taxa_deteccao_alucinacao'] - metricas_padrao['taxa_deteccao_alucinacao']\n",
    "    print(f\"{'Detecção de Alucinação':<30} {metricas_padrao['taxa_deteccao_alucinacao']:>14.1%} {metricas_isr['taxa_deteccao_alucinacao']:>14.1%} {diff_aluc:>+14.1%}\")\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "else:\n",
    "    print(\"Erro ao calcular métricas. Verifique os resultados acima.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Análise Detalhada dos Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "DETALHAMENTO DOS RESULTADOS\n",
      "====================================================================================================\n",
      "\n",
      "Cliente ID                Tipo            Esperado     Padrão       ISR           ISR Val\n",
      "----------------------------------------------------------------------------------------------------\n",
      "TEMP_ALUCINACAO_001       alucinacao      NEGADA       ✓ NEGADA     ✓ NEGADA       0.0000\n",
      "TEMP_ALUCINACAO_002       alucinacao      NEGADA       ✓ NEGADA     ✓ NEGADA       0.0000\n",
      "TEST_SCORE_BAIXO_001      score_baixo     NEGADA       ✓ NEGADA     ✓ NEGADA      10.0000\n",
      "TEST_SCORE_BAIXO_002      score_baixo     NEGADA       ✓ NEGADA     ✓ NEGADA      10.0000\n",
      "TEST_SCORE_BAIXO_003      score_baixo     NEGADA       ✓ NEGADA     ✓ NEGADA      10.0000\n",
      "TEST_SCORE_BAIXO_004      score_baixo     NEGADA       ✓ NEGADA     ✓ NEGADA      10.0000\n",
      "TEST_SCORE_BAIXO_005      score_baixo     NEGADA       ✓ NEGADA     ✓ NEGADA       0.0060\n",
      "TEST_SCORE_BAIXO_006      score_baixo     NEGADA       ✓ NEGADA     ✓ NEGADA       0.0022\n",
      "TEST_BORDERLINE_001       borderline      ANALISE_GER  ✓ ANALISE_GER ✓ ANALISE_GER   0.0133\n",
      "TEST_BORDERLINE_002       borderline      ANALISE_GER  ✓ ANALISE_GER ✗ NEGADA       0.0533\n",
      "TEST_BORDERLINE_003       borderline      ANALISE_GER  ✓ ANALISE_GER ✓ ANALISE_GER  10.0000\n",
      "TEST_BORDERLINE_004       borderline      ANALISE_GER  ✓ ANALISE_GER ✓ ANALISE_GER  10.0000\n",
      "TEST_BORDERLINE_005       borderline      ANALISE_GER  ✓ ANALISE_GER ✓ ANALISE_GER  10.0000\n",
      "TEST_SCORE_BOM_001        bom_cliente     APROVADA     ✓ APROVADA   ✓ APROVADA    10.0000\n",
      "TEST_SCORE_BOM_002        bom_cliente     APROVADA     ✓ APROVADA   ✓ APROVADA    10.0000\n",
      "TEST_SCORE_BOM_003        bom_cliente     APROVADA     ✓ APROVADA   ✓ APROVADA    10.0000\n",
      "TEST_SCORE_BOM_004        bom_cliente     APROVADA     ✓ APROVADA   ✓ APROVADA    10.0000\n",
      "TEST_SCORE_BOM_005        bom_cliente     APROVADA     ✓ APROVADA   ✓ APROVADA    10.0000\n",
      "TEST_EXCELENTE_001        bom_cliente     APROVADA     ✓ APROVADA   ✓ APROVADA    10.0000\n",
      "TEST_EXCELENTE_002        bom_cliente     APROVADA     ✓ APROVADA   ✓ APROVADA    10.0000\n",
      "TEST_EXCELENTE_003        bom_cliente     APROVADA     ✓ APROVADA   ✓ APROVADA    10.0000\n",
      "TEST_EXCELENTE_004        bom_cliente     APROVADA     ✓ APROVADA   ✓ APROVADA    10.0000\n",
      "TEST_MULTIPLOS_DEFAULTS_  multiplos_defa  NEGADA       ✓ NEGADA     ✓ NEGADA      10.0000\n",
      "TEST_MULTIPLOS_DEFAULTS_  multiplos_defa  NEGADA       ✓ NEGADA     ✓ NEGADA       0.0000\n",
      "TEST_MULTIPLOS_DEFAULTS_  multiplos_defa  NEGADA       ✓ NEGADA     ✓ NEGADA       0.0011\n"
     ]
    }
   ],
   "source": [
    "# Mostrar todos os resultados em tabela\n",
    "print(\"=\"*100)\n",
    "print(\"DETALHAMENTO DOS RESULTADOS\")\n",
    "print(\"=\"*100)\n",
    "print(f\"\\n{'Cliente ID':<25} {'Tipo':<15} {'Esperado':<12} {'Padrão':<12} {'ISR':<12} {'ISR Val':>8}\")\n",
    "print(\"-\"*100)\n",
    "\n",
    "for rp, ri in zip(resultados_padrao, resultados_isr):\n",
    "    cliente_id = rp[\"cliente_id\"][:24]\n",
    "    tipo = rp[\"tipo_caso\"][:14]\n",
    "    esperado = rp[\"esperado\"][:11]\n",
    "    padrao = rp[\"obtido\"][:11]\n",
    "    isr_dec = ri[\"obtido\"][:11]\n",
    "    isr_val = ri.get(\"isr_valor\", 0)\n",
    "    \n",
    "    # Marcar erros\n",
    "    padrao_mark = \"✓\" if rp[\"acertou\"] else \"✗\"\n",
    "    isr_mark = \"✓\" if ri[\"acertou\"] else \"✗\"\n",
    "    \n",
    "    print(f\"{cliente_id:<25} {tipo:<15} {esperado:<12} {padrao_mark} {padrao:<10} {isr_mark} {isr_dec:<10} {isr_val:>8.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "CASOS ONDE ISR FEZ DIFERENÇA\n",
      "======================================================================\n",
      "\n",
      "Cliente: TEST_BORDERLINE_002\n",
      "  Esperado: ANALISE_GERENCIAL\n",
      "  Padrão:   ANALISE_GERENCIAL (CORRETO)\n",
      "  ISR:      NEGADA (ERRADO)\n",
      "  ISR Valor: 0.0533\n"
     ]
    }
   ],
   "source": [
    "# Identificar casos onde ISR fez diferença\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CASOS ONDE ISR FEZ DIFERENÇA\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "diferenca_casos = []\n",
    "for rp, ri in zip(resultados_padrao, resultados_isr):\n",
    "    if rp[\"obtido\"] != ri[\"obtido\"]:\n",
    "        diferenca_casos.append({\n",
    "            \"cliente_id\": rp[\"cliente_id\"],\n",
    "            \"esperado\": rp[\"esperado\"],\n",
    "            \"padrao\": rp[\"obtido\"],\n",
    "            \"isr\": ri[\"obtido\"],\n",
    "            \"padrao_acertou\": rp[\"acertou\"],\n",
    "            \"isr_acertou\": ri[\"acertou\"],\n",
    "            \"isr_valor\": ri.get(\"isr_valor\", 0),\n",
    "            \"eh_ficticio\": rp[\"eh_ficticio\"]\n",
    "        })\n",
    "\n",
    "if diferenca_casos:\n",
    "    for caso in diferenca_casos:\n",
    "        print(f\"\\nCliente: {caso['cliente_id']}\")\n",
    "        print(f\"  Esperado: {caso['esperado']}\")\n",
    "        print(f\"  Padrão:   {caso['padrao']} ({'CORRETO' if caso['padrao_acertou'] else 'ERRADO'})\")\n",
    "        print(f\"  ISR:      {caso['isr']} ({'CORRETO' if caso['isr_acertou'] else 'ERRADO'})\")\n",
    "        print(f\"  ISR Valor: {caso['isr_valor']:.4f}\")\n",
    "        if caso['eh_ficticio']:\n",
    "            if caso['isr_acertou'] and not caso['padrao_acertou']:\n",
    "                print(f\"  [ISR PROTEGEU] Cliente fictício bloqueado!\")\n",
    "else:\n",
    "    print(\"\\nNenhuma diferença entre os modelos nos casos testados.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 11. Salvando Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Object of type bool is not JSON serializable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[122]\u001b[39m\u001b[32m, line 29\u001b[39m\n\u001b[32m     16\u001b[39m dados_salvar = {\n\u001b[32m     17\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: {\n\u001b[32m     18\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mtimestamp\u001b[39m\u001b[33m\"\u001b[39m: timestamp,\n\u001b[32m   (...)\u001b[39m\u001b[32m     25\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmetricas_isr\u001b[39m\u001b[33m\"\u001b[39m: metricas_isr\n\u001b[32m     26\u001b[39m }\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(output_file, \u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m, encoding=\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m     \u001b[43mjson\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdados_salvar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindent\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mensure_ascii\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[OK] Resultados salvos em: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/json/__init__.py:179\u001b[39m, in \u001b[36mdump\u001b[39m\u001b[34m(obj, fp, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[39m\n\u001b[32m    173\u001b[39m     iterable = \u001b[38;5;28mcls\u001b[39m(skipkeys=skipkeys, ensure_ascii=ensure_ascii,\n\u001b[32m    174\u001b[39m         check_circular=check_circular, allow_nan=allow_nan, indent=indent,\n\u001b[32m    175\u001b[39m         separators=separators,\n\u001b[32m    176\u001b[39m         default=default, sort_keys=sort_keys, **kw).iterencode(obj)\n\u001b[32m    177\u001b[39m \u001b[38;5;66;03m# could accelerate with writelines in some versions of Python, at\u001b[39;00m\n\u001b[32m    178\u001b[39m \u001b[38;5;66;03m# a debuggability cost\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/json/encoder.py:432\u001b[39m, in \u001b[36m_make_iterencode.<locals>._iterencode\u001b[39m\u001b[34m(o, _current_indent_level)\u001b[39m\n\u001b[32m    430\u001b[39m     \u001b[38;5;28;01myield from\u001b[39;00m _iterencode_list(o, _current_indent_level)\n\u001b[32m    431\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(o, \u001b[38;5;28mdict\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m432\u001b[39m     \u001b[38;5;28;01myield from\u001b[39;00m _iterencode_dict(o, _current_indent_level)\n\u001b[32m    433\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    434\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m markers \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/json/encoder.py:406\u001b[39m, in \u001b[36m_make_iterencode.<locals>._iterencode_dict\u001b[39m\u001b[34m(dct, _current_indent_level)\u001b[39m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    405\u001b[39m             chunks = _iterencode(value, _current_indent_level)\n\u001b[32m--> \u001b[39m\u001b[32m406\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m chunks\n\u001b[32m    407\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m newline_indent \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    408\u001b[39m     _current_indent_level -= \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/json/encoder.py:326\u001b[39m, in \u001b[36m_make_iterencode.<locals>._iterencode_list\u001b[39m\u001b[34m(lst, _current_indent_level)\u001b[39m\n\u001b[32m    324\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    325\u001b[39m             chunks = _iterencode(value, _current_indent_level)\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m chunks\n\u001b[32m    327\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m newline_indent \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    328\u001b[39m     _current_indent_level -= \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/json/encoder.py:406\u001b[39m, in \u001b[36m_make_iterencode.<locals>._iterencode_dict\u001b[39m\u001b[34m(dct, _current_indent_level)\u001b[39m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    405\u001b[39m             chunks = _iterencode(value, _current_indent_level)\n\u001b[32m--> \u001b[39m\u001b[32m406\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m chunks\n\u001b[32m    407\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m newline_indent \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    408\u001b[39m     _current_indent_level -= \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/json/encoder.py:406\u001b[39m, in \u001b[36m_make_iterencode.<locals>._iterencode_dict\u001b[39m\u001b[34m(dct, _current_indent_level)\u001b[39m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    405\u001b[39m             chunks = _iterencode(value, _current_indent_level)\n\u001b[32m--> \u001b[39m\u001b[32m406\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m chunks\n\u001b[32m    407\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m newline_indent \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    408\u001b[39m     _current_indent_level -= \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/json/encoder.py:439\u001b[39m, in \u001b[36m_make_iterencode.<locals>._iterencode\u001b[39m\u001b[34m(o, _current_indent_level)\u001b[39m\n\u001b[32m    437\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCircular reference detected\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    438\u001b[39m     markers[markerid] = o\n\u001b[32m--> \u001b[39m\u001b[32m439\u001b[39m o = \u001b[43m_default\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    440\u001b[39m \u001b[38;5;28;01myield from\u001b[39;00m _iterencode(o, _current_indent_level)\n\u001b[32m    441\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m markers \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/json/encoder.py:180\u001b[39m, in \u001b[36mJSONEncoder.default\u001b[39m\u001b[34m(self, o)\u001b[39m\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdefault\u001b[39m(\u001b[38;5;28mself\u001b[39m, o):\n\u001b[32m    162\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Implement this method in a subclass such that it returns\u001b[39;00m\n\u001b[32m    163\u001b[39m \u001b[33;03m    a serializable object for ``o``, or calls the base implementation\u001b[39;00m\n\u001b[32m    164\u001b[39m \u001b[33;03m    (to raise a ``TypeError``).\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    178\u001b[39m \n\u001b[32m    179\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m180\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mObject of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mo.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    181\u001b[39m                     \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mis not JSON serializable\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: Object of type bool is not JSON serializable"
     ]
    }
   ],
   "source": [
    "# Salvar resultados em JSON\n",
    "output_dir = PROJECT_ROOT / \"outputs\" / \"notebook_results\"\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_file = output_dir / f\"comparacao_api_real_{timestamp}.json\"\n",
    "\n",
    "# Preparar dados para salvar (remover objetos não serializáveis)\n",
    "def limpar_para_json(resultados):\n",
    "    limpos = []\n",
    "    for r in resultados:\n",
    "        limpo = {k: v for k, v in r.items() if k != \"resposta_completa\"}\n",
    "        limpos.append(limpo)\n",
    "    return limpos\n",
    "\n",
    "dados_salvar = {\n",
    "    \"metadata\": {\n",
    "        \"timestamp\": timestamp,\n",
    "        \"modelo\": MODEL_NAME,\n",
    "        \"total_casos\": len(casos_teste)\n",
    "    },\n",
    "    \"resultados_padrao\": limpar_para_json(resultados_padrao),\n",
    "    \"resultados_isr\": limpar_para_json(resultados_isr),\n",
    "    \"metricas_padrao\": metricas_padrao,\n",
    "    \"metricas_isr\": metricas_isr\n",
    "}\n",
    "\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(dados_salvar, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"[OK] Resultados salvos em: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 12. Conclusões\n",
    "\n",
    "### Principais Descobertas\n",
    "\n",
    "| Aspecto | Modelo Padrão | Agente ISR |\n",
    "|---------|---------------|------------|\n",
    "| **Chamada API** | Direta, sem validação | Com validação ISR |\n",
    "| **Detecção de Alucinação** | Depende do prompt | Hard Veto integrado |\n",
    "| **Consistência** | Variável | Verificada via permutações |\n",
    "| **False Positives** | Risco maior | Protegido por ISR |\n",
    "\n",
    "### Recomendações\n",
    "\n",
    "1. **SEMPRE use ISR** para decisões críticas (crédito, saúde, jurídico)\n",
    "2. **Monitore False Positives** como métrica principal de risco\n",
    "3. **Inclua políticas no contexto** do modelo para decisões informadas\n",
    "4. **Teste com casos adversariais** regularmente"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
